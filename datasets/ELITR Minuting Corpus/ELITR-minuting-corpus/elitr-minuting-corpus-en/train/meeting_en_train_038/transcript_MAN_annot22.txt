(PERSON7) Okay so [PERSON6], I have -
(PERSON2) [PERSON6]?
(PERSON7) I have few questions
(PERSON6) I will just close, eh, sorry.
Just a moment.
(PERSON3) [PERSON2], when do you think we can discuss -
(PERSON6) Very sunny.
(PERSON2) Any, any time.
[PERSON3] that time I could not reply you because we had that board meetings.
So the, the same day you, you did, eh, mail me and so I was not able to respond to you. I'm really sorry for that.
But, eh -
(PERSON3) No problem.
(PERSON2) But, eh, whenever you find time, anytime, I'm available anytime you say.
I, I will be there.
(PERSON3) Okay.
Okay, so I, I'll tell you.
(PERSON2) Yea.
You mail me, I'll, I'll be there.
For the meeting.
(PERSON3) Okay, okay, thank you.
(PERSON7) Okay.
So [PERSON6] I have a few questions.
So, let us just be on the same page regarding the annotation.
So first we have the ASR right? 
From the -
(PERSON6) Yes.
(PERSON7) Video meetings.
Right?
(PERSON6) Yes.
(PERSON7) Then, then we create the reference summary.
(PERSON6) Then we co- correct the ASR.
(PERSON7) Okay.
And these are the files that you can find in the data with MAN.
M-A-N.
This is manual.
So the files named: first date, then, eh no, no.
First the word" transcript", then M-A-and - it means that manual it means that manual transcript and it means that it is manually corrected ASR.
So the ASR outputs are mostly very bad.
So you can't use them for, to understand what the meeting was about.
(PERSON7) Right.
(PERSON6) So if you look at the output.
If you look at the ASR and the file name which contains ASR in the name you can just see that it is senseless.
And that these MAN are corrected, yes.
(PERSON7) Okay.
So we have that ASR first, eh, from the, from the minute, eh, from the meetings. 
Then we correct the ASR.
That is a manually corrected ASR.
Ad then, eh, the reference summaries are created from the manually corrected ASR.
(PERSON6) Yes.
(PERSON7) So who creates this reference?
(PERSON6) So the annotators worked in such way that they have got, eh, they had a video or audio file and they corrected the transcript according to that.
And then they created this summary.
(PERSON2) So [PERSON6] we have, always we have 2 annotators for, e- annotating each minute?
(PERSON6) Not always.
Eh, not always.
This is reflected in the table you have access to. 
So if you look at the table there is a colon, eh twice transcript or twice minutes.
And if there is "1" in the colon it means that it is really doubled.
That we have double annotation.
And if it doesn't have "1", if it has "0" it means that we don't have -
(PERSON2) I'm sorry.
Which, which table are you referring to [PERSON6]?
(PERSON6) [PROJECT1] minuting annotation.
[PROJECT1] minuting annotation.
I, I have, I ga- gave the link here to the chat.
Eh, if you look at the [ORGANIZATION2] chat today.
(PERSON2) Yes, yes.
(PERSON6) And click on the table, you will be inside.
(PERSON2) Okay.
Thank you.
(PERSON7) Okay, so, erm, then we have the reference summary, which we refer to as the gold standard summary that are generated by human annotators.
Right?
(PERSON6) Uuf, yes.
Ac-
Well, so why I'm so kind of unsure.
Because annotators are humans, mostly students, not always from the computer linguistic fields so creating the summary is quite subjective and sophisticated task and the summaries that are created by different annotators, when I look at them with my eyes, are very often not very similar.
So they are gold annotations.
But if two people make the same they are never the same. 
(PERSON7) Okay.
So for each manually corrected ASR.
So sometimes we have 1 reference summary and sometimes we have more.
Is that correct?
(PERSON6) Yes.
(PERSON7) Okay, okay.
So, eh, so now -
(PERSON6) So they are, eh, so we are getting new and new every day.
So I work with more than 10 annotators and they gave me back the text.
So this is the working, eh, flow.
(PERSON7) Okay.
(PERSON6) Progress.
(PERSON7) So I will also, so after generating the reference summary what is the next task?
(PERSON6) The next task is alignment of transcript with the summary.
And this is what we spoke with [PERSON4] now.
This is the tool that is created by a [PERSON1] and should be implemented somehow from the organizational point of view.
What we asked you to help ou- us with.
So -
(PERSON2) So [PERSON6]?
The one's which we have -
(PERSON6) Just a moment.
I have shared the link to this guide to this tool.
The annotation instruc- the installation instruction and annotation guidelines and [PERSON7] if you please look at it and then just think how to share the data and contact [PERSON1] and, to, eh, to start it.
So this are some technical problems.
Not tech- te- technical issues with GitHub that I cannot launch myself, I sh- I n- need some technical help with that.
(PERSON7) So you want this annotation tool to be like kind of web based kind of thing?
(PERSON6) Kind of -
(PERSON7) Well, GitHub.
(PERSON2) It is already, it is already I think.
(PERSON6) It is already written.
I needed, I need to implement the system how to share the data.
How the annotators get the files, eh, so, eh, there is some idea how to do the- it.
And [PERSON1] has written it already.
I will forward to you.
But it was written Slovak so I have it to translate it into Czech.
Eh, so, eh, so there is the idea how to implement it the -
But we should implement it.
So that I can start to use it.
(PERSON2) So [PERSON6], one thing that, the present data, which we have, it's, it is already aligned with the, with the agenda right?
(PERSON6) No.
(PERSON2) It is not?
(PERSON6) No.
We have summaries and we have transcripts.
They are not aligned to each other.
(PERSON2) Okay.
So all the data needs to be aligned.
(PERSON6) Yes, eh, eh, lines in the minutes are on not aligned to pieces of transcripts.
This is what is not done yet.
This is, so we have the tool for that.
But we don't, we didn't do it yet.
(PERSON2) Okay.
So we need to align, eh, the reference summaries to the original meetings.
(PERSON6) Yes.
Yes.
No.
Eh, no, no, no, no.
We have to align, eh, lines in the summary do transcript, to manual transcript.
(PERSON2) Yea, that means, yeah, yeah.
I did understood the correct version I think.
(PERSON6) What?
(PERSON2) Yes, I did understood the correct version I think.
(PERSON6) Hmm, okay.
(PERSON7) So is, so it is just alignment between the ASR and the reference summary.
(PERSON2) Not ASR, the manually corrected one.
(PERSON7) Yea, that's, that's actually an ASR
(PERSON2) After the ASR output, after the ASR output we manually correct that.
So that's -
(PERSON7) Yeah.
But eventually that's an ASR right?
(PERSON6) We don't have any alignment -
(PERSON2) <unintelligible/> [PERSON6], eh.
[PERSON6] I think better to make a documentation of it.
So that -
(PERSON6) What we have and what we haven't?
(PERSON2) What we have, what we haven't and we e- what you need from us.
(PERSON6) So this is what I actually deed.
This is what you have in the document named, eh, that one.
This example.
Yours.
That I sent to you, you always refer to.
That on- 
Just moment.
I will find it.
Qualitative analysis of meeting minutes.
(PERSON2) Qualitative analysis?
(PERSON6) Yes.
(PERSON2) Qualitative analysis of -
(PERSON6) Yes.
So if you -
(PERSON2) <unintelligible/> in that document?
(PERSON6) <unintelligible/> analysis.
I will share it.
(PERSON3) Eh, sorry guys, eh, can I leave you for now? If you don't have anything -
(PERSON6) I think yes, we just, eh, solve this technical details.
We are -
(PERSON3) Okay.
To see you, but really it -
(PERSON3) It was nice to see you.
(PERSON6) It was nice to see you [PERSON3].
Always.
(PERSON3) Okay, thank you.
(PERSON6) Bye.
(PERSON7) Thank you -
(PERSON3) Good bye.
(PERSON7) Bye. Bye.
(PERSON3) Bye
(PERSON6) Bye, bye.
So I have, eh, sent the link to qualitative analysis again.
And if I look at it, eh, this is wha- what is given. 
So is, this is actually what we have.
(PERSON2) You are referring to qualitative analysis document, right?
(PERSON6) Yes.
(PERSON2) So, eh, then, eh, but there we just have one example, which is, which are steps how we create the entire data, right?
(PERSON6) Hmm, which example?
No I just, in that qualitative analysis document.
I mean, the piece given, given is what we have.
(PERSON2) So what we need to do is, eh, is to make alignment of the manually corrected transcript with the reference by annotator A and annotator B.
(PERSON6) Yes.
Not, we, well actually not really you but annotate might -
(PERSON2) Yea, yea annotator.
(PERSON6) With some technical helps of [PERSON7].
(PERSON2) Hmm.
(PERSON7) So, eh, [PERSON6] can you just explain this one?
Like, eh, like example one.
So first is the ASR transcript and then there is a manually corrected transcript and then there is original minutes.
So do you mean the original minutes by the reference summary?
(PERSON6) Well no I actually, I did, eh, eh, so do I have it now.
So for some minutes we, eh so from some meetings we also have original agendas.
Not generated by annotators but real.
Like the one we have now.
So I will, eh I, I will extend this part with a, the descriptions of the data.
So I will extend the description of the data.
Now.
And I will send it to you.
Yes I, I, we have to do it, maybe in the, if you are okay with that I will extend in this qualitative analysis document.
(PERSON2) Yes please.
(PERSON7) Yes.
(PERSON2) I think you will have to -
(PERSON7) So -
(PERSON2) ??
(PERSON6) Because we have full minu- minu- minutes -
(PERSON2) <unintelligible/> good to me as well.
(PERSON6) Because actually we, I have everything here but without the names of the files.
Minutes to these meetings.
We have original minutes completed by meeting organiser or secretary.
This is what is what is marked in the files by "ORIG".
ORIG.
The name of the file.
(PERSON7) So where are you typing [PERSON6]?
(PERSON6) Yea, yea, in this -
(PERSON2) In the qualitative analysis document.
(PERSON6) And additionally created minutes by the annotators twice for some of the meetings and this has "GENER" in the name of the file.
(PERSON7) So you mean B and E like, generated, minuted by annotator A and annotator B are the reference summary.
Is that correct?
(PERSON6) Eh, eh, annotator on- A and annotator B are all generated summaries.
This is, this "GENER".
(PERSON7) So where is the reference summary?
Which one should we concern that -
(PERSON6) Okay, maybe I don't understand something.
What is reference summary?
(PERSON7) So, I can see when mentioning like that manually corrected ASR to reference summary alignment.
So what does that mean?
So which one is the reference summary <unintelligible/>
(PERSON6) Once more. 
What is reference summary? 
I didn't get it.
Re- what -
(PERSON7) So we, so we were talking <unintelligible/> meeting about something called reference summary and candidate summary right?
So which one is the reference summary <unintelligible/>.
(PERSON6) Ahaa, okay.
You're speaking about experiments.
Eh, this is what we should decide.
So we have just 2 different summaries, both of them are generated by eh, people, both of them have the same quality.
And, eh, eh I think it's on us actually to decide what should be reference and what should be candidate.
Doesn't -
(PERSON7) Right.
Yes.
(PERSON6) These are just -
(PERSON2) So candidate is something which is autom-
Checking my connection.
So candidate is something which is originated from automatic minuting.
We don't have anything from automatic minuting in my example.
So, eh.
(PERSON2) So -
(PERSON6) Everything that i say now -
(PERSON2) So in, because, eh
(PERSON6) Automated is kind of goal.
(PERSON2) [PERSON7] ,eh, [PERSON7] is referring to candidate and reference.
So reference once, one though by annotators and the candidate<unintelligible/>.
(PERSON6) Yea okay.
So the, a- all, all my data is reference summary.
(PERSON2) Automatically.
(PERSON6) Are reference summaries.
So we have a- in I -
(PERSON2) Yes.
(PERSON6) Ideally i have 3 variants of references.
One of them is original minutes that is generated by the secretary or meeting organizer and 2 other reference summaries are generated by different annotators, eh, and both of them have kind of the same value and I never know which of them is more reference which of them is better.
I actually can't even say if, eh, files with "GENER" which are generated by annotators are better or worse than original minutes.
Because it's also, it also depends on their meeting and on, eh, the annotator and so on. Sometimes, eh, the meeting organizer makes good agenda and it, eh, eh, it could be used as a gold data, but an- a- on the other hand sometimes they just write one line that we disgui- discuss, eh, the book.
And so the original agenda just contains one line, we discuss a book.
And, eh, the generated, eh, summaries contain two pages of different detailed descriptions on which chapters is currently, has been currently described.
(PERSON7) So.
Okay.
So, I think now, eh, we are picking up consensus that we have the manually corrected transcript and then we have the summaries generated by humans.
So the thing is that, why we do so?
Because we ne- really need to have something as a gold standard.
While we done our task.
So, this is also I guess one of the, eh, 5 minute transcript that which one to consider as the gold standard summary.
Because if we have multiple summaries, eh, at least one you should have against which all the other task, or all the generated, eh, automatically generated minute has to be compared.
(PERSON6) So we made, eh, more than one, eh, minutes, because this is a very subjective test for, and we did it for the analysis.
So just really to get the information which it just have a few of them that are double.
Most of them are just single.
And, eh, eh, eh, yes, and, eh we did it double to see how different they are and maybe to search some, something common between them, eh, and to f-, s-, to fi-, so this is for different task, not for shared task.
For shared task we should really have just one gal-, gal-, galf-
(PERSON7) Right.
(PERSON6) I understand, yes.
But if we want to think what minutes are really good, what minutes, what, eh, what, is, what is in common between this, so when somebody summarizes the me- meeting. 
We should have more variants.
So this.
And maybe it may, it may also come, hmm, interest when real can follow the evaluation on the alignment, as we spoke about it now with [PERSON4].
So this is ki- kind of different task yes.
(PERSON7) Yeah, yeah.
So the thing is that if we have too much variation between the human generated reference summary, then it will be hard to establish our dataset as a gold standard one.
(PERSON6) Yes, yes, beca-
(PERSON7) And maybe it would be silver or gold standard.
Yeah.
(PERSON6) Yes, the variation is really great.
It's really extremely very, very big.
[PERSON2] I think you have seen that.
(PERSON2)  Yes, yes.
(PERSON6) So you actually -
(PERSON2) Saw experiments as well.
It, it's the same.
(PERSON6) Yes.
(PERSON2) And even if you manually look at them, they are different.
(PERSON6) Hmm.
(PERSON7) Yeah.
(PERSON2) But, eh-
(PERSON7) So this thing-
(PERSON7) [PERSON7], [PERSON7] <unintelligible/> we, eh, we have gold standard as our reference summary by annotators.
That's the best quality we have.
From our data I guess.
(PERSON7) No we, we, we can't call it gold standard and, th- and this seen that has really passed some, eh, test measures.
Because, eh, you see, like, eh, gold standard is something where multiple people will agree, but here we see that eh, that the, that the minutes that have been generated by different people are hugely vary.
So call it the gold standard would be too early, but, eh, we need to see.
(PERSON2) I'm saying that this is the best quality we have.
The one which is generated by a- annotators.
This is the best quality we have.
So if one want to compare anything.
(PERSON7) So which -
(PERSON2) You, we have -
(PERSON7) So if we have 2 minutes which one we will consider as the best?
For the same meeting.
(PERSON2) That's what is something which I'm working on right now.
(PERSON7) Yeah, so -
(PERSON2) [PERSON6] did went?
(PERSON7) I don't know.
Yeah.
Maybe she is not here.
(PERSON2) Yeah.
So this is something I was saying that we ca- eh, I wasn't <unintelligible/> as a gold standard but this is I think the best quality we have in our dataset.
(PERSON7) Yeah.
Yeah we have to -
(PERSON2) The one which is referenced.
So they're not.
Yeah so not all of them are by 2 annotators.
Only some of them are by 2 annotators.
(PERSON7) Yeah.
So -
(PERSON2) But that's a good thing.
But that's a good thing to have, that.
(PERSON7) My only worry is that when we release the data with the shared task, how qualitative, how we can establish the quality of that.
That is an important thing.
Otherwise, like, people will not move that far with us. 
(PERSON2) Yes.
So what do you think what we can do for that?
(PERSON7) So, Yeah.
So [PERSON6] I will just discussing, we were just discussing -
(PERSON2) What we can do for that.
(PERSON7) That.
Hello [PERSON6] are you there?
(PERSON2) I think she is connecting right now.
Wha- what are you suggesting what we can do for preparing the data for the shared task?
How can you prepare the data for the shared task?
(PERSON7) The thing is that, eh, if we, if we just did only 1 minute and say that this is the minute from this given, given thing.
So we need to think of ways to convince the community that this minute is to some extent reflecting whatever is there in the task.
So manual evaluation of the generated meeting is important that's what -
(PERSON2) That will be -
(PERSON7) Like.
Yeah go ahead.
(PERSON2) So okay.
This will, this will help us to manual evaluation framework will help us to make it gold standard.
I didn't even thought of this point.
This is really good point.
(PERSON7) Yeah.
(PERSON6) Manual evaluation of, eh, manual meetings you mean?
(PERSON7) Yes.
(PERSON6) Yes.
(PERSON7) Manual evaluation of manual meetings will -
(PERSON2) And also, eh, [PERSON6].
Yes.
(PERSON7) Will help us to say that, okay so this has made some annotation standard and maybe we can call it the gold standard or silver standard dataset.
Because eventually we are going to, we have to like, eh, share the data with our participants.
Another thing [PERSON6] i want know from you.
So, while we organise the shared task we have to like, w- wherever will be our participants we have to share the data with them. 
So what are you thinking about the ethical considerations for the, to share the data.
Yeah.
(PERSON6) Um-hum.
(PERSON7) Because you, you do a shared task, you have to make your data available, like.
(PERSON6) You mean, eh, so which data you mean?
Should we also provide, eh, audio files o- on- only chats?
(PERSON7) No, no.
Only the transcript and the, and the, and the manually corrected AS- the manually corrected -
(PERSON6) Yes, yes, yes.
(PERSON7) Only the manually corrected ASR and the reference summary and the gold standard scores that we generate.
Eh, we say that okay these are the gold standard scores for each of these minutes.
So this 3 data have to release.
Yeah.
(PERSON6) Yes.
Eh, well, eh, this is eh, well i-, hm, this is not solved definitely yet because we have some ethical forms given to participants that, eh, some of the- somebody, so for, for some data we have signed the consent that agree with the open use of the data.
Eh, for some data we have kind of oral confirmation that it is okay and so that we are kind of sure that people will sign us, eh, the pa- the consent with use of the data.
Eh, for some, ah, for some of Czech, of this Czech data I, I will go to them next week and we will also thing is, speak about.
So this is kind of, work in progress too and.
But eh, actually, eh, how, ehm, happily we, most of our data comes from [PROJECT1] meeting and [PROJECT1] meeting are us, so we are, we agree to -
(PERSON7) Yes, but, eh, but I need a, so this is an important part of the proposal.
That we have to specifically mention that we have obtained consent from the multiple parties in the meeting to release the data to the shared task participants.
(PERSON7) So this is one -
(PERSON6) And up to when it should be ready.
(PERSON7) Eh, up to, up to the proposal deadline.
So maybe sometime in November we should have the agreement signed from all -
(PERSON6) All of the agreements.
(PERSON7) From all of the participants.
(PERSON6) Should they be signed on the paper so we have [ORGANIZATION1] forms signed at the moment?
(PERSON7) Eh, I think, I think like consent over [ORGANIZATION1] form, or maybe, maybe you can just prepare eh, electronic doc, maybe in some PDF.
(PERSON6) And eh, it shouldn- should it be explicitly stated that eh, the data can be used for the shared task.
(PERSON7) Yes, yes.
(PERSON6) Yeah.
(PERSON7) That they have to agree to that.
That should be explicitly specified in that ethical form that we are going to launch the shared task.
(PERSON6) So there should be a special, a, a special form actually.
So we should find all the people and they should sign that they are not -
(PERSON7) And we have to anonymize the data.
So suppose like we are talking.
So, eh, if [PERSON4] and [PERSON6] and [PERSON2].
So it should be like participant 1, participant 2, participant 3.
(PERSON6) Hmm.
(PERSON7) So we have to release that, the data in this form.
So there -
(PERSON6) Hmm.
(PERSON7) Should, there should be anonymity and that clause, maybe before, eh, before, eh, sending the ethical document to the, to the parties import we may have to show them a doc where there is the, where there is a transcript with the name of the participant and the anonymized version of the transcript.
So if they are okay with the anonymized version to release to the public, or to the participant for our shared task.
So this is one, eh -
(PERSON6) Woa, sh- what shou- we should show what to him?
(PERSON7) So maybe, eh, maybe, ah, y- we, oh, while you send the, send the ethical consideration form of their consent that their data should be released, eh you can show a snippet that, well you name won't appear in the meeting, you will be anonymized as participant 1 or participant 2, we just -
(PERSON6) So the- this information just to put it into the consent?
(PERSON7) Yes, yes.
(PERSON6) Yes.
Hmmm.
Hmmm.
(PERSON7) Another thing, eh, another thing I should mention that so- in, in the, in the automatic transcript many a times, eh, we, like, we mention the name of the person so that would come inside the dialog.
So we need to properly anonymize that person.
(PERSON6) Hmm.
Okay.
Eh, I will, eh, make the, for consent form.
May I show it to you before I give it to participan-
(PERSON7) Yes, please.
Yes, please.
Please do it.
Yes.
(PERSON6) Hmm.
Okay.
(PERSON2) And one more thing is coming to my mind.
I don't know, eh, whether we need that or not.
But if we just talk about that same point.
If you want to show that our data is of gold standard then probably, eh, we, we also have to make an interface for that for making the manual evaluation.
What do you say?
Eh, there is some disturbance.
(PERSON6) Sorry I have some disturbance around.
Could you repeat it please?
Sorry.
(PERSON2) Yes.
So I was saying that, eh, the, eh, we were just talking about.
So eh, eh, [PERSON7] <unintelligible/> suggested a very good point that to make this as a, eh, go- eh, gold standard, eh, data.
To make our data which we have, eh, we have collected, eh, as gold standard, just to prove that it is a gold standard data.
Eh, we have to, eh, pass it through some manual evaluation.
So it fits to certain parameters.
And so for that I think we also then have to design some interface for it to make ah, to make the ratings of -
Because the data size is, is huge.
It has 80k or 100k.
80 to 100k for English.
So each of the minute and, eh giving some rating to it won't be that easy. 
And I think an interface for that will really help.
Just to show that this is a go- gold standard, eh data.
What do you mean by interfaces?
A setca- a set of, of questions?
(PERSON2) As, eh, yes.
So maybe like the one which [PERSON1] has developed for, eh, the, eh, the already, eh, thing.
Already the data, I think alignment and all.
So same interface for this as well.
For the manual evaluation guidelines which will be develop-
(PERSON6) Hmm aha.
Aha.
(PERSON2) So, rating for that.
So this will appear, the data will appear and the people are going to rate it.
So, just on the gold, gold standard measures.
So we would set some key points.
And we will say that these are the five key points where we have a-
(PERSON6) Oh, oh.
Do you suppose that they were re-evaluated it, eh, comparing to the transcript or without using the transcript?
(PERSON2) So I think, eh -
(PERSON6) So if we -
(PERSON2) Maybe acting w-, we should consider transcript.
But transcript -
(PERSON6) So if they don't -
So, hmm.
(PERSON2) If we, if we take transcript and we also show the reference summary by annotators, it will be very difficult to map them and give ratings for that.
(PERSON6) Eh, but if you, eh, eh, well.
Realistically if you don't have a transcript and just look at the minutes at the refere- at the summary.
Eh, you actually, if you <unintelligible/> manually created summary you don't really know if it is good or bad.
Because it is always -
(PERSON2) Aha.
(PERSON6) Readable it is always kind of coherent.
It looks well.
It looks like there are some important points.
(PERSON2) Mm-hmm.
(PERSON6) Eh, so,y- ye- an- then you have to subjectively estimate if it is good or bad.
And this will be very, er, kind of unreliable bit- because it depends on if you like longer or shorter descriptions, if you don't like that they begin with a small, low case or something. But if you take its, eh, from another rave<unintelligible/> you, n- use the transcript then this is quite a big work to tr- to estimate -
(PERSON2) Yes.
(PERSON6) To, to, to ma- to estimate it.
(PERSON2) Because -
(PERSON7) Yes so this -
(PERSON2) Transcript will be a <unintelligible/> size.
So it will be multiple pages.
(PERSON7) So this point
(PERSON2) Yes.
(PERSON7) Yeah.
So this point I want to redirect with, eh, [PERSON6].
So, eh, this is a good point to define the manual evaluation thing.
But, eh, we should also at the same time keep track of the facticality aspect that we need to do this by December.
(PERSON2) Hmm.
(PERSON7) So this evaluation should be there in our background that we have to do this.
(PERSON6) Should -
(PERSON7) And we have to evaluate our meetings
(PERSON6) Eh, maybe.
So that's why I thought about that task for [PERSON2] for making, n- fo- for comparing meetings, eh -
(PERSON2) Hmm.
(PERSON6) For the same, so minutes for the same meeting and, eh, eh, contra to minutes for different meetings.
The s- so, so it also could, this automatic measurement could also help in that respect.
So if that -
(PERSON2) Hmm.
(PERSON6) Eh, eh, if you have, eh, bad, eh, if you can distinguish them it, hmm, with higher number, ye, it, it says something about the quality of the minutes versu- hmm, maybe it could be also used.
Can't it be?
(PERSON2) Yes, yes, it can be.
But, eh, to be very specific I think, eh, we have two, eh set some parameters for that.
If you talk about manual.
(PERSON6) Well and about manual, eh.
(PERSON7) Because, you know, like, if we have to do the manual annotation again, on the minutes, like if you set some parameters and you ask the annotators to annotate them again that whether, this falls into some quality standard or not, it will consume time.
So I'm absolutely part- partitude this, but at the same time I want to leverage on whatever we already have.
Eh, because -
(PERSON6) So you think that now we should set up a set of questions that will help us evaluate manual minutes.
(PERSON2) Yes.
(PERSON6) Then we should give them immediately, to annotators.
So they, they start as soon as possible to evaluate, eh, many minutes.
Let's do it.
Yes.
We can do that.
(PERSON7) Yes, right.
(PERSON2) And even, eh, [PERSON6], I think formulating the, the guidelines of manual evaluation will also help if we first give the transcript and the summary to annotators that's maybe from the linguistic point of view can see, eh, what difference is there in-between them.
So all those points can also be mapped.
Because we already have annotators.
And maybe we can frame those cautions based on that.
(PERSON6) Hmm, okay from this, eh, aha, one, well, well yes, but, eh, we don't have it implemented.
So we don't have even implemented the first, eh, interface, eh, and we don't have the second at all.
And [PERSON1] is, so will not tried it, immediately, because he has lot different thing to do.
I just tought that we could do, eh, what we could do, if, eh, immediately, as far a- as, soon as we can, implement this [PERSON1]'s guidelines.
(PERSON2) Hmm.
(PERSON6) Mhhmm about guidelines, interface.
And start giving existing meeting to annotators, eh, with 2 task they can align it and, eh, estimate it.
Because if they are already aligned they will read it and they will see the minutes and they will have the estimations immediately in mind.
So they will have a good estimation.
(PERSON7) Like -
(PERSON6) Could we do that?
Should we do that?
So let's, let's do, eh, so let's, eh, start this alignment as soon as possible.
(PERSON7) Yes, so <unintelligible/> to the, I will look into the interface and see how it works, and -
(PERSON6) Hmm.
(PERSON7) Like, might be, like some of <unintelligible/> choose that it, it has, it exists and are we calling to [PERSON1] for this?
Maybe set up a meeting with him.
And I will get back to you o- on this very soon.
(PERSON6) Okay, hmm.
(PERSON7) Yeah, yeah.
(PERSON6) Okay.
(PERSON7) And the thing about the, the manual validation.
So if you could do it.
I think it will take time for, to really frame up some good questions that, what constitutes a good minute.
(PERSON6) Hmm.
(PERSON7) So, eh, so the thing is that, eh, my only concern is whatever we do, eh, at least for the first shared task we need to freeze our pipeline, or our work flow very soon.
And not deviate from that and do the first shared task and keep our, eh, our like other ideas also going in parallel but for the shared task I think we need to fix things very soon.
(PERSON6) Hmm.
(PERSON7) So that our annotators, because I <unintelligible/> data and, yeah.
(PERSON6) Yes.
And maybe, eh, is [PERSON5], [PERSON5] are you there?
(PERSON5) Yes I'm here.
(PERSON6) Eh, eh, as you started to write this manual evaluation protocol I saw some questions already there for the evaluation of, eh, minutes some as <unintelligible/>, topicality, grammaticality, clarity, fluency.
Eh, what, where do they come from these, eh, estimations?
Is it, i- i- is there some state of the art how to measure, how to estimate it?
So I also have found some other estimated questions.
If there are something that we should know, eh, before we, eh, create this questions.
(PERSON5) Well that's, eh, what I have read when I did, I have done some research on these things.
Like I have some papers about evaluation and -
(PERSON6) Hmm.
(PERSON5) That is usually the criteria that they use.
(PERSON6) Hmm.
Eh, so -
(PERSON5) To <unintelligible/>, to <unintelligible/> is the way we should we-
(PERSON6) So -
(PERSON5) I will -
(PERSON6) Sounds -
(PERSON5) It's not complete yet.
(PERSON6) Yeah, I see that.
So, so this actually what you begin, we have to finish now this cr- this questions which to ask.
(PERSON5) Yes I will detail them.
I will, because I -
(PERSON6) Yeah.
I have seen also some kind of text understandability, is the maybe, maybe some overall exte- expression, should, should it be there?
Shouldn't it be there?
So I, I don't have such experience as you have.
(PERSON5) I will finish it.
I will detail it.
(PERSON6) Okay.
Okay.
Okay.
(PERSON5) I will just finish it in a few, eh, today or tomorrow and then we can -
(PERSON6) Okay.
Perfect.
Perfect.
Thank you.
(PERSON5) But now please just send me the path of the data set, because I'm not able to find it.
It's very messy, the minut- [PROJECT1] minuting path in the classerer has become very messy so.
(PERSON6) But you know been there.
[PROJECT1] minuting corpus and then.
[PROJECT1] minuting corpus original recording and within that, all of it.
Okay, but you should also put it in the spread sheet.
(PERSON6) Eh, which spread sheet?
Eh.
Well I will e-mail it to you.
I, I okay. I, I see.
Okay.
(PERSON5) You can just put it there and I have the link obviously.
(PERSON6) Yeah. I, I, I did it.
(PERSON5) Okay.
Thanks.
(PERSON6) Ah, the [ORGANIZATION1] spread sheet, I will.
Okay.
Hmm.
Okay.
So fo- we can stop for now yes?
I think.
(PERSON2) Eh, eh, could you, eh [PERSON7] <unintelligible/> did you set up, eh, the SSH?
(PERSON7) Yeah, yeah I did.
(PERSON2) You did?
And, eh, did someone contact, I think O- [PERSON4] already did e-mail regarding that.
So we don't have any meeting right now, right?
Or is it something which is scheduled for now?
(PERSON7) Eh, I think we have the normal coffee hours.
(PERSON2) And is that, eh, something scheduled for the demonstration of SSH?
(PERSON7) I don't know -
(PERSON2) So [PERSON4] -
(PERSON7) I guess.
(PERSON2) [PERSON4] already actually e-mailed regarding that.
(PERSON7) Yeah, so they will, like reply right?
So this will confirm.
(PERSON2) Eh, yeah.
(PERSON7) Otherwise I'm fine with accessing, yeah.
Okay, so sounds we are on, eh we are a little bit clear if not at all and okay, so.
Let's meet tomorrow?
(PERSON6) Hmm.
(PERSON2) Okay.
(PERSON6) See you.
Thank you.
(PERSON7) Okay, so, yeah.
Thank you.
(PERSON2) Bye.
(PERSON7) Bye, bye.
(PERSON6) Bye all of you.
Bye.

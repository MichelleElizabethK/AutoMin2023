(PERSON17) Now and I'll share my screen to show you the the [ORGANIZATION7] document, for while, sharing. 
 Yes. 
 So this is this is the [ORGANIZATION7] doc, this is the agenda, hopefully, you all are looking in it. 
 And and uh, since I have [ORGANIZATION4] and also [ORGANIZATION3], 
I would like to remind everyone uh, that we need to provide [ORGANIZATION2] with the details of the technical set up, so description of cables that uhm like will be connecting to, together with the graphical map, how to be connected to the interpretation booth. 
 And uh, this is, this is still something for the very far future for the May, uh, 2020. Uh, but [ORGANIZATION2] is already doing the preparations. And this description is necessary because this is what they will, uh, be giving to the companies to translation. Uh, an interpreter agencies,  uh, so that they know what what to connect to. So [ORGANIZATION2] is seeking for the technical description, that their other uh, like subcontracted companies will will connect to. 
 So please, please do provide this as soon as possible to to [PERSON9]. So that [PERSON9] can share it with [ORGANIZATION2], And and [ORGANIZATION2] can, can use it in their preparations. So. Can [ORGANIZATION4] and [ORGANIZATION3] please confirm that they will provide the details. And the graphical map like what needs to be where, some first draft of this obviously. But this is important so that the technical negotiations can start, please confirm that you will provide this, you have the details the detail questionnaire in your email already for some weeks. Maybe we haven't heard some- <other_yawn/>
() To when do you need that layers?The description? 
(PERSON17) And I think as soon- I can hear myself and now in in the code that the little bit confusing, uh, but I know that for the May 2020 event, they are preparing everything with very big like time- very had- a lot of a lot of time in advance, like two way to advance I would say. 
So for the workshop that we are also preparing for that event,  they want to have a schedule of  the workshop like a detailed plan till uh the end of this week actually,  and that's for workshop,  which will happen in May 2020. And this is for the workshop to like be- be accepted to the to the congress. 
And for these technical preparations. 
I think they will already start negotiating with the possible translation agencies  or interpreter agencies, and they want to include this description into this call, because they need to do the official call for tendors  and the the soon that they get the detail the the better I would say. So, please,  please do it till the end- Okay. 
() Yes, okay, 
(PERSON17) Yeah, ok, thank you. Uh, then another thing uh I would just like to be sure that it will be [PERSON3] will be coming for the Fair in March. I mentioned this in my previous email and this is just- so that we know <other_yawn/> it's a good hotel in reasonable distance to the uh, to the- 
() Yes still taking here who is coming that somebody from [PERSON12] you will come, it's better, not completely <unintelligible/> [PERSON3] or somebody else. 
(PERSON17) Yeah, okay, So then remember to- remember to tell uh, and uh, coordinate the hotel <unintelligible/> with her.  So she- she can assist you with finding a good hotel, or or whatever, yeah. And then for the management questions. I would like to know what are the technical differences between the [ORGANIZATION3] platform as [PERSON5] saw it when he was in [ORGANIZATION3]. Uh, and the [ORGANIZATION4] platform,  which will be used for most of the events of of the [PROJECT1] project. 
So I would like to thank [ORGANIZATION4] for describing their, uh, their platform from the like API point of you. So we have access to the code. And the big question is now whether, this is the same API that we need to connect to, if we want to use the [ORGANIZATION3] platform and them all for the March event. So I don't know if it is [ORGANIZATION3] <other_yawn/>  
() So, from [ORGANIZATION3] it should be the same platform the only thing is we should do some testing if it's really, but from the <unintelligible/> point of view. It should be exactly the same platform. We just since we haven't connected our [ORGANIZATION3] workers to [ORGANIZATION4] since the last project. 
We have to make sure that it is still running,  but we assume that it is exactly the same. 
(PERSON17) So just to clarify. You may be answering the the converse question. So for the long term, we definitely want to use the [ORGANIZATION4] platform and connect [ORGANIZATION3] tools to that,  and also our tools to that,  and and all that,  but for the very short term very near event in March. I think that it is easier to use the final presentation platform like the uh, the the the video mixer and all this from [ORGANIZATION3]. 
And my question is then whether this platform that run demo that [PERSON5] saw in [ORGANIZATION3] was the the [ORGANIZATION4] platform,  or whether it is some different [ORGANIZATION3] platform. So in other words. 
If we use the [ORGANIZATION4] connectors and connect and wrap our machine translation with these connectors, will we be able to use them in March with the [ORGANIZATION3] final presentation in the video mixer? 
(PERSON15) Well, do you hear me? <other_yawn/> 
(PERSON17) Yes. 
(PERSON15) Yes, the EPI are the same as the the one used in the [PROJECT2] project. Then in my opinion it would be the same. 
(PERSON17) Okay, yes. I'll try- 
() Yes, that's true. The only thing I say that we have to make one final test if it really works,  but it should all be exactly the same. So that shouldn't. You should be able to connect the same way to [ORGANIZATION3], or to [ORGANIZATION4]. 
(PERSON17) Okay, yes. That's very good. So, we will now review the uhm the details of this  and will coach the connector,  so that we can uh, use our machine translation with this sample connector, uh. And will also work on the ASR  and for the ASR  we have two options, uh it seems that uh, with our experience from colleagues,  who have left the department,  but not have [LOCATION1]. We could get Kaldi models,  running in and operational for March. 
Uh, but for those we would also needs to ride the connector. If we are able to uh, to finish what [PERSON5] has started in [ORGANIZATION3], and if we are able to train [PERSON7] ASR  then obviously you have the connectors ready. 
Would you have any comment on what would be the preferred thing for March? So I don't hear any any comment. So it probably depends simply on us. So we'll, we'll start with connecting the machine translation. Uh, and if we like, if it's it's easy to use this connector. 
Then we will also consider doing the Kaldi ASR , uh but I still for the March enterprise, I still hope more for the [PERSON7] training for [OTHER5] ASR, okay, and now uh,  the thing that I call it full replica,  and [PERSON7] has call it like a final final check or  one last quick check. We would really like to uh, to have the system running  way sooner than than March comes. Uh, and that is so that we can test, how our MT like behaves,  whether we need to finetune some times out or whatever,  uh, so that it it behaves reasonably for that. And we also need to uh, use this set up to get, to to train the the assistant  that we will have fo- will have for the fair,  so if you remember the fair is organized by high school students for high school students. Uh and one of the high school students, we we know his name, we have met him, will be there to to help us in in running all that,  obviously not the technical things about the uh, the ASR and its translation,  but only like the set up the physical set up and also, uh we have the extra competition plant that every participant whose speech will be recorded,  and and translated will also transcribed their own speech,  and we will evaluate the clarity of their speech through standard ASR measures. So we need to uh like fin- finalize the set up in which will be recording, what they are saying. Showing the subtitles. And also, uh, uh, have access to the uh, to the recording,  so that they can do the transcription. 
And and uh- As uhm as last reason we would like to have a backup system running in case of some network issues or or whatever. So my question is whether a whether we can uh- Well. Now, I see that the [ORGANIZATION3] and [ORGANIZATION4] platform are the same thing actually, uh, so uh- so the question is,  can we get it running on our side? And what what should we do for that? 
And if you have any reasons why we should not be allowed to to run the platform on our side on our machines, then can you provide access for both users, and also modul providers now so that we can test it well had,  and we have already some uh, information for from [ORGANIZATION4]  but I'm still not able to interpret this because I haven't read the documentation of the of the connector. So back to the original question. 
Uh, how difficult would it be uh, uh, to get the full system running on our machines? So, [PERSON15], I don't hear you, if you are saying anything, I can not hear anything. 
(PERSON15) Yes, I'm, I'm just re-reading the four topics. <laugh/> 
(PERSON17) Yeah. 
(PERSON15) Ehm-  I think that we have to reason a little bit more about this topic. Sorry. 
(PERSON17) Yeah, okay. So so then I see two reasons why getting a full replica running in [LOCATION1] would be difficult. One is like the licensing things. So that you- you don't want us to to have it. 
And the other would be technical things that it is a software,  which is like a build in pieces and it's not too easy to uh, to get it running, uh. So which which of these reasons are the the more important ones? <laugh/> 
(PERSON15) Uh, probably the the second have everything working with the require a lot of work. But maybe it's better to talk about these topic with [PERSON14] which unfortunately is not here today,  I know,  <laugh/> but we- we will discuss about it later. 
(PERSON17) Yeah, yeah, obviously. So so that is obviously not the point of of these like training session today,  get the mediator running. That will be another session for this needed. And first you need to get the to discuss this with [PERSON14]. Okay. So I see. 
And they the fall back,  then should however be possible. And maybe the fall back is something which is going to be resolved after they the training today,  if we are not running the mediator the that the hard of that system ourselves. How do we get, uh- how do we connect to the hard running at at [ORGANIZATION3] or or your servers,  
(PERSON15) Well, you can connects to the [ORGANIZATION4] mediator, which is the one we provide you access. You have just to use the given URL and the given ports for worker- 
(PERSON17) Yeah, okay. So this is this is- 
(PERSON15) <other_yawn/> - running and you can keep I think. 
(PERSON17) Yeah, okay. And what what like tools what machine translation systems. And ASR systems are already connected there. If any? 
(PERSON15) No at the moment it's completely uhm empty. 
(PERSON17) Empty. Okay. Yeah. So we w- for the uh- So this is good for testing whether our API works eh in principle. But it is still not sufficient for full test ehh for the March event. So what should we do as a full test for the March event? And this is also question for for [ORGANIZATION3], whether uh like they have this mediator running elsewhere, uh,  with the appropriate ASR  and and the the [OTHER4] ASR, and [OTHER3] ASR, and and their [ORGANIZATION3] [OTHER3] [OTHER4] translation system uh running or uh, if uh- so if if they will provide us with similar connection details for their mediator. Uh, or if we should use this mediator, which is now running at at [ORGANIZATION4] and [ORGANIZATION3] should connect their components to this, and we should like set up everything around uh, this mediator. So what what would [ORGANIZATION3] prefer? 
() So I guess the first suppose that was we could [OTHER3] [OTHER4], [OTHER4] [OTHER3] system. To the mediator <unintelligible/> [ORGANIZATION4] and we make my connect our website of that we see that works. 
(PERSON17) Okay. That would be excellent. There will be test as [PERSON8] said that would- test whether the current version of the mediator is compatible with the connectors as as you have them  and we will connect uh to that to, uh,  when we have our translation system wrapped,  and we'll see how that cooperates,  and and how it doesn't recognize pieces and so on. Uh, so I'll- I'll wrote down here that [ORGANIZATION3] will uh, connect their tools including the web presentation, uh, to this and let us, let [ORGANIZATION1] know so that we can test it out,  right? So this is this is very good. And I think that this,  this particular [ORGANIZATION4] mediator,  which is already running will be the the the heart of the live system that will be used in March, right? Can can we rely on that? Is that like big enough? 
(PERSON15) Yes, maybe URL, URL and the porter may change, but we will provide uh a platform, don't worry. <laugh/> 
(PERSON17) Yeah. And and that is it will be the very same version of of this. 
(PERSON15) Yeah. 
(PERSON17) And a technical thing, if we- So there is still many things to be- like discussed and prepare for the for the fair in March. Uh, but one of them is that there will be probably more sites like more locations at the fair, which will need this, or which will benefit from this translation. There is the main stage,  and like the secondary stage. And there is also a chance that we'll have our booth,  and that so for the booth,  the idea is that we would have an offline system. 
If [ORGANIZATION3] has has, uh, set up- physical set up one notebook that runs everything. Uh, that people can see on the spot, that would be nice. Uh, we know that the Internet connection would be limited only to some places and the at the venue. So this. One the booth presentation should not to rely on any Internet connection. And then the one or two places will obviously need Internet connection. Uh. So if we have two sides that are interpreting things. Uh, will we need two such meditators,  or is it the the that one such mediator can do choose independence streams of of translation? 
() Should be one mediator. We just need several workers then so several MT and ASR workers. 
(PERSON17) Okay. So within one mediator you can easily like set up that the subtitles coming- that the speech signal coming from one microphone or one source gets translated. And and uh, and produce to one particle output channel of this mediator,  and the same mediator can handle another channel. Of another uh, audio source to another uh, like final display, web display platform, right? 
() Yes, normally you have several sessions, so you would have then two sessions, they are all displayed like. There is like than one website which just play the whole sessions, and people can select which session they want to follow. 
(PERSON17) Yeah, okay. Yeah, that's that's good. So we should be with this particular mediator,  or maybe a replacement one. If if uh the the different port and and different IP address. Uh. We should be set up. Uh. We just need to connect the various tools,  and we can play around with that. 
And uh, one last thing is the recording capacity. Uh, I assume that [ORGANIZATION4] already has some- it's not a worker. It's actually like a sink. Uh. So uh, some, uh, some connect- something some component that will connect to the same mediator, uh,  and it will record the audio signal to a file on on a disk. Is such tool, does such a tool exist? 
(PERSON15) Well, actually, we have some <unintelligible/>, let's say worker which performs audio recording,  but we share the last time uh that, I I don't remember, who has to develop more stable components to record the audio. 
(PERSON17) Ehm. Okay. So so this is something that we should include in our tests,  but it's existing in principle,  and it should be easy to uh, to get running. 
(PERSON15) Okay. 
(PERSON17) Yeah. Okay? So thank you very much. So this was everything from the like organization point of view from me  and I would like now to hand out the uh, the presentation to [PERSON15] for all the details about the platform they they want to share with us.So I'll stop sharing my screen. 
(PERSON15) Okay. And I'll share my screen.Let's see. Share documents. Okay. Just give me one second, please. Okay, pretty good. Okay, do you see the whole slide,  or just the half? 
(PERSON17) I see post of the slide. I see like the center. You can maybe,  yeah, yeah, I was zoomed that out,  and there is also one option to fit page. So this is the best you can get. We'll always have a preview of the upcoming slide, unfortunately. 
(PERSON15) Well, I think that we are all technician,  and we can aset also this <laugh/> this presentation- 
(PERSON17) If you want to underline something, you can even click the the the draw and then possibly you can uh- There is an option to to use the marker and draw on the slides. 
(PERSON15) Thank you. 
(PERSON17) Maybe you saw that for for a second. 
(PERSON15) Are we ready? 
(PERSON17) Yes, please. 
(PERSON15) Everyone? Okay,  uh, first of all, welcome to everybody, I'm [PERSON15], I'm senior software developer in [ORGANIZATION4]. Actually, I'm a Java developer. So please be indulgent with my C programming skills. <laugh/> Okay, today I will try to explain you the- Okay. Thank you, wau.Assistance. I will try to explain you the [ORGANIZATION4] service architecture idea. We will have a work view uh of the system and then we will go trough ex- Let's say flow chart explaining the the flow of the interactions, and as more example of code. Okay. The the [ORGANIZATION4] service architecture is implemented as a client service, client server architecture, clients connects to a mediator which distributes the request and the load amongs avalaible workers. <unintelligible/> scenario. 
Clients are the <unintelligible/> that sends the data and the eh the workers represent different call components like ASR, SLT, MT and so on. And the- usually client in service architecture sends and repo- Actually uhm usually ehm clients in [ORGANIZATION4] service architecture both sends and receives data. In our scenario clients will just send data  because processed data with the publish on other platforms. 
But uhm let's keep it simply for the moment  and assume that clients both sends and receives data. Well, you you know,  such kind of infrastructure is a distributed one, ehm it has really convenient API which hides protocol complexity,  and it's completely asynchronous and integration will be based on callbacks implementation. The clients start, oh well. The PDF export was unfair on this slide. 
I'm sorry. But actually, the image was exactly the one in the previous slide. The clients start the service request by specifying the type and the language of the output stream. And by specifying the type and the language of the input stream. For example the caption use case the clients specifies [OTHER3] UK audio stream as output and [OTHER3] UK as-Oh my gosh. Ehm, where's the example? Just a seconds, sorry. Okay,  I'm back. Sorry for moving the documents. [ORGANIZATION4] service architecture client and worker interaction, connection is based on message exchange,  clients connect to the service architecture.  
They will send media streams like text, audio, image and so on. The subscribe to a service specifying their output type, output fingerprint. and requires a service specifying their input type and input fingerprint. For example <unintelligible/> use case the clients specify uh [OTHER3] UK audio as output stream and [OTHER3] UK captions, let's say, as input stream. Workers instead register to this service architecture with one or multiple services that I, they are able to handle. For example I'm able to perform ASR uhm from for the [OTHER3], [OTHER1] [OTHER3] language. Uhm. Each worker accept just one incoming service request per connection. And in order to make to be sure to uhm process all the incoming request,  there exists some queue for processing all the incoming requests. 
Okay. When we talk about language actually we talk about fingerprints. Fingerprints are used to specify the exact language and the general media. They are pretty pretty simple,  they need 2 letters for language code optional 2 letters for country code and again optional additional strings specifying the uhm the domain of uhm of the stream. Uhm, the domain it's- it's important for example for ASR workers  in order to uhm describe, for example, the domain of the language model used in the ASR process. Usually the language part might be sufficient. But just to know that there the exist this possibility. 
Okay and we talk about input and output types. Those are the stream types,  which I use to specify the type of the media, for a fully specified input or output media stream both fingerprint, and type must be specified. And well, supported type streams are audio, image, text and unseg-text, which is the one used by ASR hypotheses. Well and this is the uhm decor of the the problem of managing client requests and worker services. The mediation problem. If a client declares for example,  it wants to output [OTHER3] audio, and [OTHER3] audio, let say. [OTHER3] is aaa [OTHER1] [OTHER3] is the output fingerprint. 
Audio is the output type. And it request [OTHER3] unsegment text as input fingerprint and input type, the mediator must find one or <unintelligible/> of multiple workers that are able to converts audio containing [OTHER1] [OTHER3] speech, into [OTHER3] unsegmented text. For example the [OTHER3] ASR worker is able to accomplish this kind of request. 
But this is the case that just requires one worker,  the mediator,  which is the core of the [ORGANIZATION4] service architecture is able also to um accomplish more complex requests concatenating different workers. Well, messages between the mediator and the client and workers are transport the Xml packets. We have a specific type,  you will find the types described in the M cloud XS the file which is in the uhm [ORGANIZATION4] platform sample code on Github. 
(PERSON17) May I have a question about the packating. So, so far you have talked about the stream. And now you talk about packets, packets. So uh, what is the granularity in or is it up to the the user? Uh, if you have the input audio stream who does the segmentation? I think there's a a a quite a bit of logic that can affect these decisions. Uh. So uh, we need to be sure that this is done reasonably. And there will be lot of like options, to to experiment with, so that the experience is the best for the user. Can you tell us more about this? 
(PERSON15) Yeah,  the real time use case is a little bit complex, but in the batch audio file processing for example uhm- you, the client is in charge of splitting audio files into bytes and send them using for example, the data packets. And when it test finished sending all the data it will send the the done message, we will see in in a minutes. To declared that he has finished sending all the all data. Is it clear? 
(PERSON17) So I understand that the 2 modes of operation. One is for the batch processing,  and one is for the online processing. And in general,  most of your presentation will be for the badge start processing, uh, because- But but hopefully we'll later also get to the details of the of the online style of processing.  
(PERSON15) Today we will keep things a little bit simple but in the [ORGANIZATION4] platform report you should find example also to- ehm, let it works like a real times stream. You will find the sample coder in the in the report. 
(PERSON17) Yeah. Okay. So this is something that uh, like someone knowledgeable should should double check for us. When we are implementing the connector so that we use the the correct demo for the online processing and not for the batch processing,  and and and so on. So this is important that- 
(PERSON15) Oh, don't worry. In the in the code you will find a lot of comments, detailed explaining you exactly ehm, eh  all the possible options,  don't worry. But in any case we can provide you some some support, of course. <laugh/>  
(PERSON17) Yeah. Thank you. Yeah, so for we as [ORGANIZATION1] will need both set ups, will be doing both the online things for the [ORGANIZATION2] events,  and also the document level like document processing. And this is something that we still need to decide whether we will make use of the platform or not. This is uhh unclear uh. But if we do use the, do use the platform then uh, then it will be a batch processing. 
So we'll need both eventually poss-, or possibly, uh. And I was also curious about the uh, the practical limits. When you talk about the batc- batch processing, uh. So is it okay, to feed in four hours or eight hours of of recording through this,  or will- so it it could lead to files, larger than 4 gigabytes on the platform, machine, and so on. So what are the practical limits of this.  
(PERSON15) Well actually, practical limits are uhm space left on device. <laugh/> You know, yet, maybe to uhm to reason about uh. 
(PERSON17) Yeah, so before we do this. We should definitely talk to you. But in principle, there is no uh like arbitrary limit such as the, there used to be some fall systems that couldn't handle files larger than four gigabytes, and and so on. So you are no longer affected by this. 
(PERSON15) Oh, well, my opinion it's strictly related to ASR, for example, worker. And I think that there is no limits, just memory and space on disk. 
(PERSON17) Yeah. Okay.Thank you. 
(PERSON15) Yeah, you are welcome. Okay. We were talking about packet types uhm. Okay. Here we can see the packet type. We have data packet which contain exchanged data. And data may be audio, text or image. And then we have a couple of status packet types, which are done sent by the client to when all the data has been sent. Error when sent when occurs an error of course. Reset, sent whenever the client, or the worker should reset if- initial state, usually it depends on at the end of uh conversation.  
And flush, which is sent whenever the client, or the worker should finalize the processing which is the common flush eh method. Okay. And now we talk about how the initial handshake for client and the I think for worker immediately after the spot. Yes. Okay. This is our the client present itself to the mediator,  it test to declare it's output stream first of all. Okay. Actually M cloud at the flow description. <laugh/> 
I have to say it's <unintelligible/> you will going to use M cloud out the flow description to method, but basically it is used to uhm sent a some human readible description of the of the process is going to start. For example, weather forecast of 27th June 2012, for example. After the flow description is specified, the client has to connect to the mediator  and then it's specify its output stream description,  using this M cloud announce output stream. And it's specify is output type and output fingerprints. And also us <unintelligible/> which can be the some stream, some unique ID which is used to uniqely identify the stream. 
Actually a client may specify more than one output stream, but it will not be our case. Okay, and- and if the client as also to require some input which is not always the case I have to say, <unintelligible/> test to specify the input type and the input fingerprint of the input stream. So using the M cloud announce output stream it declares the output stream credential. Let's say, then uh the output type and the output fingerprint using M cloud announce using M cloud request input stream it instead the declares the desired input type and input fingerprint. Ookay. Well, uh curiosity let's say, client is allowed to request multiple different input stream for one stream. Maybe, for example, he wants to send uhm in [OTHER1] [OTHER3] audio, and wants to receive the [OTHER6] text translation. 
But also the [OTHER1] [OTHER3] caption fo example. On the other end a worker is require to register to the mediator  providing the information about the server uhm providing. The information the service can provide, okay. Okay,. <laugh/>  I repeat. The worker register to the mediator, providing the information about the services. He can provide, slightly better. Okay. A worker may be able to offer multiple services but only one service can be active at the same time per connection. Okay. This is an example of the client application. Uh, I don't know if you are able to read it well,  but actually is the- 
Okay, thank you. Is the ehm the program flow in the documentation. Okay, this is the typical client implementation. Uh, for example in the [ORGANIZATION4]s, in the Gitlab report, we share there a file which is the client <unintelligible/> example,  which is probably the uhm most complete. Okay, the client first creates the M cloud a structure after starts.  And the structure is used to keep all the necessary information for the data exchanges between the client and the mediator. Okay, see- oh  well,  M cloud create is the <unintelligible/> as function  you will use M cloud create too.
And there exist also the possibility to establish an SSL connection with M cloud create SSL too. Uh, I'm sorry the slides are not the updated. But fortunately, the API documentation is pretty complete. Okay,  after M cloud create, it adds flow description,  specifying the name and the description of the flow that is handled by the client. It connects to the mediator that is running on a specific host and port in our example mediator dot [ORGANIZATION4] dot IT. 
And I don't remember by heart to the port but it's written, don't worry. Hm, okay. When the connection is established the client can announce the output stream. Uhm. And then if an inputs stream is required which as I already told you, it might not be the case. Uhm sometimes clients just want to upload things,  and they doesn't care about results,  but if that cas-, in this case uhm, when uhm it cares about results. It test to specify input type and input fingerprint calling M cloud request input stream. Okay. Then I don't like that graph that much. <laugh/> I have to say,  in order to be able to receive and process packets. 
While sending packets. The the process is a little bit complex I have to say but it's really well described in the sample code then don't worry. Ehm, of course, if you want to both sent and receive you have to do everything asynchronously. So for this reason uhm data are put in queue, the uhm the data that ha- that has to be send are put in the sending queue. And the data that arrives are put in the processing queue. When he test finished sending all the data,  you have to call the M cloud to with finish, which actually sees everything is manage asynchronously. The main thread populate process uhm the sending queue. And you actually don't know when it has finish to sen- to send  all the data,  then you use these method M cloud with finish to wait until all the data has been sent. 
If something was wrong, you can use another method which is not written here but hopefully is writing next slide which is M cloud brak. Which is called by the error call back. And this use to stop sending with the remaining data. As I told you also the packet ehm the received packet uhm are received asynchronously. And the data are <unintelligible/> in the processing queue. Uhm. If ehm okay. We are here. And the ehm keep waiting for data and heel and done message arrives from the mediator. Then the clients can disconnected from the mediator. And that's it. <laugh/> Please note that in order to process a new media stream and service request, and your connect needs to be established, and this is the client flow. Let's have a look little bit to the ehm callbacks and queues involved, okay. Okay. 
Okay, we have two main queues processing queue and sending queue. Mm, sending queue is used for sending messages. Okay.  <laugh/> Easy. And packets are appended to this queue for sending. In case of any error the callback function ehm set by- you see, it's a little bit small but under each queue you have the ehm callbacks that can be set for this queue. And under the ehm, okay <other_yawn/> it's really small, I'm sorry. Ehm, okay sending queue you can set M cloud, this are the method names, you can set the error callback and the break callback. In case of error ehm the ehm error callback is used, and ehm when there some- something to be handle the ehm <unintelligible/>, mainthread,  and you have to ehm stop sending ehm data the brek call back is used. 
If the clientes request the an input stream, ehm the resection of packet should also be done in a seperated trade. In this case by calling ehm req message asink, you see it on the top of the slide. 
Ehm this thread simply wait for incoming packet and eeh take specific actions eh depending on the packet type. This, this will be aconstance eh switch among packet types. In case of data messages the packet is appended to the processing queue, in case of done message ehm M cloud wait finish is called. In case of error or reset message ehm it stop the processing using M cloud the break. For the processing queue- here. Ehm we have different- for different ehm callbacks which can be used. Which are data callback called the for each packet in the queue sequentially. Here you put ehm your handling of the input stream. 
Finalize callback which is called the mediator signals that all the worker involving service request ehm have finish processing the data. This means that the clients safely, can safely disconnect ehm from the mediator. And of course you know,  error call- callback for errors,   and break callback when mcloud break has being called. Really simple. Okay. Worker example program flow, which is I think a little bit simpler. Ehm, in the <unintelligible/>Github report refer to the backend ASR to example which is probably one of the most compleats. As the client to the first thing that the the worker do is to prepare the mcloud the structure.  
Which keep the necessary information about ehm for the data exchange between worker and mediator.  Then it adds service description that are offered by the worker by specifying the attention. Worker specifies input type, input fingerprint,  but also output type and output fingerprint. They need connects to the mediator,  that is running on a specific host  and the ports. And the it  waits for ehm client requests,  and teh when a requests arrives it waits for the next packets, which of course the first time it waits for the first packet. Ehm which arrives from the client stream,  ehm and this time too it performs the friend actions depending on the message type. And here we have the split let's say based on the message type. If an error occur or arrives are reset message, the processing is stopped. 
 It calls the mcloud ehm break, 
 the connection ehm is terminated  and the worker waits again ehm for a new client connection. 
 In case of done message it ehm the mcloud wait finish is called, in order to let worker wait until all pending ehm packets in the processing queue have been processed. 
 And then again, the connection with client is terminated,  and the worker waits for new client connection. 
 In case of data message the packets are simply appended to the processing queue, and the worker waits for the next packets arrive. 
 Ehm, in case of flush message. 
 Ehm, the worker waits until all appending packets had been processed by calling the mcloud wait finish. 
And then it sends a flush message informing the next worker to flush it's queus as well. And then again it turns to wait for new ehm new client connection. Ehm, and that's all I think. Please note that <unintelligible/> to the client the worker always keep the connection with  mediator. Ok. 
And here we have again ehm worker queues and callback overview. Ehm available callbacks for the processing queue. Our data callback which is the function call the for each packet in the queue sequentially. You put here the data processing limitation, after processing you have to send immediately processing data in order to keep the latency lot. And the data are send by appending them to the sending queue. T
hen we have finalize callback, which is called as soon as the mediator <unintelligible/> that uhm the client as finish sending the data, again error and the break. And that's it. I think. Yep. Ehm, just a note about break callback. Ehm, remember to reset all the queue since the connection is finished and then you have to free your your data structure. Okay. The API also provides ehm here say convenience function. Ehm, in in particular interesting are the ones uhm used for prepare packets for sending ehm which prepare uhm the packet to be send based on your data type really useful. Just a couple of things regarding uhm uhm more technical details. Uh. Okay, here you have GetAttr and SetAttr of specific feature. Okay. GetAttr and SetAttr. 
Okay this one is  much more interesting,  which is word token. Uhm in order to be able to pass additional information,  which usually are are required by ASR  or machine translation use case. You can prepare packets containing array of word. And these allows you this is a word token the data structure. You can see it allows you to specify both start and stop time for example,  as well as confidence,  and you can use uhm- You can prepare your data structure, uh, using word token,  or what token array if a  more words of course, and then you can prepare uhm your packets to be sent using mcloud <unintelligible/> from word token. And A means array,  you can use just also for  well known for word token. Uh, okay. Packets also contains time stamp which are used, so let you be able to align the media stream produced by the workers,  with the media stream provided by the clients. This is used for, for example,  to sign subtitle to original audio. 
Time stamps are  in the leaders of each packet, uhm and can be used uhm - they have different  meaning based on the packet type, when you send uhm.  Okay. Time stamps are both in the packet header and in the word token array. Uhm time stamp in the packet header are absolut time stamps within the media stream. And instead word token array time stamps are relatives time stamps,  based the on the starting position of the media stream, of course you have to sign with a media stream uhm sending word token array, the time stamp use in this situation ehm, will be relative to the beginning media stream. 
Okay, we provided you all the simple code ehm there are a lot of example ehm in particular I like the <unintelligible/> client example and the ehm backend uh ASR too which are two example of client and worker. The Git repo contains the uhm the binary form of the C-library, header files, documentation  and lot of examples. Here you have the connection data to connect to the [ORGANIZATION4] mediator. You can choose to establish SSL or not connection is the same, let's say.  Let me-  not of course the same but ehm both are working. Now, maybe it is better to go through uhm an example of code. Actually I see reported the documentation examples but I don't like them that much. And maybe I can try to share with you my uhm my desktop  and see the working examples. Just give me one second. Ok. Now, you should see my- Unfortunately I'm not on my PC <other_yawn/>  but you should see my notepad plus plus editor. Here, we have- okay, thank you. 
Here we have ehm the backend ASR to example  which is the one provided in the Github repo. You can see there are lot of comments uhm then it should be pretty easy to go through the implementation. Okay, let's start. Okay, some utility. Okay.  Here we have the- Are okay. This  worker performs a kind of ASR processing actually it performs nothing. It just simulates the ASR handling of data. You will see it just sends back silly text for receiving audio data. Here we have all the callbacks implementation. In it callback, data callback which  is the one used to <unintelligible/> the ASR processing, <unintelligible/> the callback <unintelligible/> sample ehm in initialize your service. Data callback you put here your processing of incoming data. Here perform some reason about ehm time and so on. Hm, hm, hm. 
(PERSON17) And the machine translation worker will be essentially the same thing,  except that if it the messages will be of different type,  and the byte that you receive will be interpreted differently. 
(PERSON15) No. Well. The power of this solution is that,  ehm it will be exactly  let's say you have just to put ehm your implement- service implementation in to the right callback, and manage the the uhm the service <unintelligible/>,  for example what to do when error occurs or when all the data has been send, maybe  you want to let's say update the data base. I don't know. And everything is always the same. Here we have silly ASR process. But the idea is that you will put the machine translation service implementation in to the data callback for example. Okay. Here you can see that after the uh silly processing it prepares the data to be sent using <unintelligible/> function. We see in this slide. Then use the mcloud send packet <unintelligible/>  in order to feed that the queue.
And this was the data callback finalize callback it's pretty empty. Break callback in this example ehm we- the nothing. I also in the error callback ehm we perform nothing but for example you  want to uhm  print a log or perform some kind of action to manage the error. Or to report the error and so on. And here we have the main function. Okay. And these here we have the important part. Here the mcloud object is created. It prepares uhm data structure. Adds, the service than and shake with the mediator. <unintelligible/> telling that he he is able to provide a specific service. Populates the callback passing ehm specific function and in some case also specifying the processing queue. The the queue related. Then it connects, to the mediator. 
And here actually it starts  the processing if we have- Here we have uh, <unintelligible/> cycle. It waits for a client, waits for next packet. And when packet arrives, it switch between packet types. Then mcloud data arrives and it adds to the processing queue. Ehm, arrives mcloud flush. And then it waits for uhm processing queue and then sends flush to the other uhm to the other workers. Done and so on. And here, this is logic I strongly suggest it because it works really good. Wait for packet switch among data types and implement your logic. Okay. Okay. That's it. <laugh/>  And this is everything you need  uhm in your worker. Uhm now we see the uhm client by directional example,  which is also in the uhm in the Github repo. Okay. We have seen the worker. Now we see the clients parts. The <unintelligible/> example is a little bit complex speech- because is is really flexible. It will allow you to try to send really a lot of uhm it will allow you to uhm perform a lot of different use keys uhm test. 
Then the call is a little bit more complex. But we will try to go through. Through it. Again you have callback function,  which is are not those one. But are- Okay, this are just utility function writes at the end. Okay. And you we have a data callbacks, yes the callback implementation. Here you put uhm uhm what to do when uhm packet arrives, in this example you for example can write a CTM or write a plain text,  or a lot of other things. For example. Okay. Uhm each function is implemented in a different callback  and based on uhm user common line option, uhm the right callback is import. 
Again, finalize callback, break callback, error callback are pretty empy- empty because if just an example but in real production example of course you have to end all these kind of situation. Here we have <unintelligible/> message asign.  Which is the function I told you in the client program flow which is not uhm a library method but is an approach you will see in our examples. And basically uhm again it waits for uhm next packet switch among data types, packet types. That's it. Here we have the main. But important is the <unintelligible/> orchestration which is really simple even if it's put among all these line of codes. 
Here we have-. Common line options. Okay, ok, here you can see it will try to uhm simulate real type mode. For example. Okay. But here is the the beginning of the handshake. Creates mcloud object. Uhm add flow description. Then it connects to the mediator,  announce its output stream, this is used to display on add displey service which is not up and running at the moment, but it's the same. Callbacks are set here. Those are set for the sending queue, and based on the specified parenitars also the callbacks for the processing queue are set. It in its the packet type based on the use keys chosen. 
After the emit there's always the send of the packet. And when all the data are sent uhm wait finish is used to wait that uhm. Well, when all the data are queued on the sending queue wait finish is used to wait until all the data are send. When all the data are send, this connects. And I think that this is everything. Okay. And that's all. We are going to prepare also some kind of support uhm side. For example on our support system. At the moment I'm not aware about the um, the um specific e-mail to write but will provide you as soon as possible these information. Uhm I have finished. 
(PERSON17) Okay. Thank you. So I'll now present my screen. And just you show 
(PERSON15) I'm sorry, I know that they are a lot of technical information, but please don't be scare. The documentation is really complete and there are lot of uhm commented example in the repo. 
(PERSON17) Yeah, thank you. Uh. So I would just like to scroll the the [ORGANIZATION7] document that everybody has shared. One important thing that I have found is that we have a list of deadlines for- that's not really for the fair for the student fair. It's more for the [ORGANIZATION2] events that we are going to to run later on. And this document also mentions that the description of the cables and so on is something which will need earlier than in uh,  by the end of the week. The deadlines say that we would like to have this uh, by February, the fifteens. So in three days from now. So I have just assigned this to [PERSON8] <unintelligible/>,  if [PERSON8] could to this quicker. Uh. That will be great. Uh, and then, um, I have tried to capture all the details. So in the meantime. So we already have the meditator running uh,  and ehm then, uh, we've agreed that [ORGANIZATION3] will connect their tolls to this meditators for. 
And for the fair I have summarized what we will need one or two sessions,  um and uhm [OTHER3] uhm [OTHER5] and possibly [OTHER4] ASR, so the [OTHER5] is uh, to be, provided by [ORGANIZATION1] if we actually manage  <laugh/>  manage that. Uhm. So from [ORGANIZATION3] I would like to have the [OTHER3] ASR and [OTHER3] [OTHER4] MT,  and the web presentation form in this in this mediator as soon as possible like at your earliest convenience. And again I've assigned this to [PERSON8], but feel free to re-asign, I don't know who who is the one at [ORGANIZATION3] who can who can add uhm workers to uh, to the system. And uh, I've also mentioned that we'll need the recording client. So [PERSON15] please uhm provide us with the recording clients  so that we can also connected to this mediator,  <other_yawn/> 
(PERSON15) The recording worker example, is a ready in the uhm [ORGANIZATION4] uhm sample code. But I strongly suggest use it just an example. And to prepare something much stable. 
(PERSON17) Yeah. So and you mentioned that you have some stable version somewhere or not? 
(PERSON15) Not, we agreed that someone we agreed the last time that someone else will prepare the stable version,  unfortunately, [PERSON14] is not here,  and I don't remember the right information. 
(PERSON17) Okay. Yeah. So uh, um-hum. But this is not stable yet so- Yeah, I'm not sure whether we will have the capacity to to do this properly. So is there any chance when you talk to [PERSON14] that [ORGANIZATION4] will have the capacity to to prepare the recording client to test it properly? 
(PERSON15) Oh, I think that it's not on on the schedule, sorry. Maybe you have to ask it directly to [PERSON14]. 
(PERSON17) Hm, okay. So we'll  see because this is this is important, we needed for for the fair. 
(PERSON15) Thank you. 
(PERSON17) And we also need it in the longer term for uh, the recording from the [ORGANIZATION6] platform. So I hope the [ORGANIZATION6] team is still listening to this. Because this is the way that we'll be doing the recording from the uh, [ORGANIZATION6]s uhm meetings,  and we for the project- according to the project time line, I think we need to start recording by the end of March  or something like that. So that actually <unintelligible/> with uh, with the fair uhm quite well,  but we need to have it done. Yeah, so this- So. This is something bending um  
(PERSON18) [PERSON10] from [ORGANIZATION6] I have it right now. Um, did you just say that the audio recording is required for the for the fair? 
(PERSON17) Sorry, I couldn't hear you, your voice was too quiet. Quality was good  but it was too quiet. 
(PERSON18) Okay. Is it better like that? 
(PERSON17) Yeah, Thank you. So what was your question? 
(PERSON18) Um. Did you just say that the audio recording is required for the fair? 
(PERSON17) Not for the fair. Well. Yes, for the fair, but not for a <laugh/>  well, it's difficult. So there is several use cases. And one of the use cases is that at the fair  we will have- but [ORGANIZATION6] is not that much involved in the fair. So that's why you you don't need to worry. But at the fair we will have people presenting their countries or whatever.   In ninety seconds like little pitches. 
And each of these pitches will be held in [OTHER3] by non-native speaker. So we made it more interesting for the student participant by saying that it will be competition for them in terms of uh, pronunciation quality. And we will record their speeches. They it would be subtitles on the flies,  so that others can can have the support from from that subtitling. But after the student finishes their 90 seconds, they will go to the backstage  and they will hear they own voice. And they will manually write down the transcript,  and we will then use standard ASR measures to measure the quality how much they, how much the ASR departed from their own transcript. And it is fair competition,  because it is fixed length of of speech, uh,  and it's themselves transcribing their own voice and they don't know what the ASR has made with that. So uh, they can not cheat and and like artificially introduce errors that the ASR introduced. 
So for this reason we need to record what they are saying,  and the recording can obviously be done like independently of that platform as the fallback solution. But I think it would be better and easier to do it directly on the platform to have one of this recording worker connected to that. So that's why we need the recording for for for the fair. And the other use case is that we, for the minuting approach uhm for the remote meetings. Um. 
We need to record data and uhm the call such as this one that we have the internal [PROJECT1] calls uhm are also a nice source. And also we have some other possible groups of people remotely communicating  and for this we need some platform for like remote calls, that has the recording capacity. And since the I [ORGANIZATION6] platform is the one that we've agreed to use for for the project. The sooner we have the recording in the [ORGANIZATION6] set up in some way uh, the sooner we can start using [ORGANIZATION6] instead of this Adobe connect thing, for example. 
So that's- and we have a a milestone for this, in the in the proposal. So that's why I would like you to to have the recording there. And if I'm not mistaken the, the recording capacity of the [ORGANIZATION6] is scheduled for March, which coin- which almost coincide with the fair uhm recording capacity, and recording in um in [ORGANIZATION6] can be done in various ways. And one of the possible ways is that you will connect to the [ORGANIZATION4] platform,  which you will have to do anyway,  and then the recording will be done by the [ORGANIZATION4] recording worker. So this is what we've discussed early when we were like planning how to do the recording the exist way for you. So this is what we thought was the existed. I don't know whether it is still the easiest option now  or if there are any easier options. 
(PERSON18) So as to my knowledge, we agreed that we would do the recording independently for now. 
(PERSON17) Okay. 
(PERSON18) This is what we are currently working on. 
(PERSON17) Okay, yeah. So this, maybe maybe this some negotiation that you had also directly with [ORGANIZATION4] and I was not involve in that. 
(PERSON15) In the- sorry. In the shared uhm ehm examples, there is the audio recorder worker example,  which could be eventually, a good starting points,  
(PERSON17) Ehm. 
(PERSON18) So you must know,  we have our own audio set up an audio service- 
(PERSON15) Okay. 
(PERSON18) Which which just output the the audio that's being, you know, spread across all members  or all participants,  dump the audio out put into a file. 
(PERSON17) Mm-hmm.  Yeah. Okay. 
(PERSON18) And these files would end up on AWS as three. 
(PERSON17) Ehm. 
(PERSON18) And we will- we would make it available for. 
(PERSON15) Okay. Cool. 
(PERSON17) Okay, yeah. So this is an independent. <other_yawn/> 
(PERSON18) <unintelligible/> the connection to the um mediator and this would be more complex for us. So as a first step we thought, just dumping audio into a file would be easier to get um,  audio recording up and running. 
(PERSON17) Yes, this is this is very good for the project purposes for the recording of the meetings. So this means that you will be. It will be easy for you to make the the March milestone,  and we could start having our calls within the [ORGANIZATION6] platform that's excellent. Uh, I have just one technical question about this. You mention that you will be dumping to one file uh. 
(PERSON18) No, not exactly it would every participant would have its own. 
(PERSON17) Excellent. So we don't have to do speaker <unintelligible/>. 
(PERSON18) Yeah, so we have a different streams for each participant. 
(PERSON17) That's that's very good,  and and they are synchronized. So, you know, which- like they all start at the same point, the time stamps are are  global time stamps. 
(PERSON18) That I actually don't know. 
(PERSON11) Anyone hear me? Does this work? 
(PERSON18) Yes, we hear you. 
(PERSON11) Yeah, we have time stamps but don't know if they um it, but it a real time stream. So if there is no data. No, if there is no nobody speaking at a moment I don't know be data so there is no time stamp for this. Timeframe. 
(PERSON17) Yeah,  So what what do you need to have for the minuting processing later on is like these sequence,  or overlabs in which the the people were talking obviously. So it's great that you have separate streams that that frees us from doing <unintelligible/>, speaker <unintelligible/>. Uh, but we also need to know the sequence as it was <unintelligible/> by the participants. And for this. We need the global time stamps. 
(PERSON11) And um, as we are <unintelligible/> for the real- for the later when we send it up at [ORGANIZATION4], it will be realtime stream. 
(PERSON17) Yes. 
(PERSON11) So, it could contain time stamps yeah. But the recording redo is ehm like it not content time stamps. 
(PERSON17) Yeah. So this is this is something that that should be probably added. Because otherwise, I don't know how it would like <other_yawn/> extract the original overall. <other_yawn/> 
(PERSON18) Could, could we do a continuous by the recording? Like over the whole meeting? 
(PERSON11) The the problem is, we are not <unintelligible/> at that point. And if someone is not speaking, we have no data to write down in the file. 
(PERSON18) So we would need an <unintelligible/> script of file. Which carries time stamps. 
(PERSON17) Yes, that would also do the work. Like if you had  if you had some mapping between the global time, and the whatever positions in in the various channels. 
(PERSON18) But we would end up with on a on lot of audio files, [PERSON].  
(PERSON11) Yeah. 
(PERSON18) So right now, I'm thinking, whether it would be better to to go the direct way and have look at the mediator recording. But we can talk about this. 
(PERSON17) Yeah, yeah. So this this fill free to discuss directly with [ORGANIZATION4] and decide whatever is easier and more appropriate for you. Uh, and I'll have someone look at at the recording worker example,  and I'll also try to talk to [PERSON14], because I I think that it would be the easiest. If if [ORGANIZATION4] had the capacity to <unintelligible/> the recording worker,  but it will still be quite simple recording. So um, one drawback of going through the platform. If I understand correctly is that it will already do the uhm the the mixing of the channels. And if we save the separate channels. Uh. We will have a the the great benefit of of avoiding speaker diarization. So I think  for the processing from the processing point of view,  it's it's better to have separate channels. And somehow like work around with a time stamps than to have perfect mixed channel. Uh, that <unintelligible/> great time stamps  but is mixed and and you can not separated the speakers and it anymore in it. In an easy way. 
(PERSON11) I assumed third resend still send each speaker separately to the [ORGANIZATION4] platform. 
(PERSON17) Yeah, so this is something that's uhm  maybe [PERSON15] can uhm can answer. I don't know how uhm how the platform handles separate channels because so far with briefly touched upon the the concept of sessions. Uh, but we haven't touched upon the concept of channels. 
(PERSON15) Well, actually, the platform doesn't manage. If you are talking about the the service architecture platform, doesn't manage channels.  It just manage audio streams. It's in charged to the worker and the client to manage uhm eventually, uhm audio channels. 
(PERSON17) So to handle channels. You would use your descriptors. You would somehow like label the different channels with the speaker ID or whatever. And, you would have to have as many recording uhm workers as there are speakers, right? 
(PERSON15) Well. I'm not so aware of the diarization use case,  but for in case of automatic speech recognition,  ehm you prepare uhm worker who interacts with the automatic speech recognition engine, which requires streams.  And if different uhm cl-channel management is required then is in charge to the automatic speech recognition system to managed it. Uhm the service architecture just manage uhm streams  and it's not a whereabouts uhm uhm audio channels, let's say. 
(PERSON17) Hm, so what <unintelligible/> set up? 
(PERSON11) <other_yawn/> 
(PERSON15) Sorry? 
(PERSON11) So we would create a stream per user? 
(PERSON15) For example. 
(PERSON17) And technically you will have then one mediator,  and you would have uhm ASR system uhm well- 
(PERSON15) Yeah. Uhm. The mediator is only- the mediator is always just once uhm and then in charge to uhm manage the interaction between client and workers. Uhm, we haven't go through the mediator uhm topic today, because I I was thinking that it was not uhm on the the [PROJECT1] project interest. But uhm actually it just prepares worker uhm it search for workers path, in order to accomplish client request,  uhm and the mediator is just one,  uhm many workers can connects to the mediator. A worker can pro- can manage uhm just one connection per time, then if you have to manage two streams  for example in the, at the same time,  you have to be able to run two workers. 
(PERSON17) Yeah. 
(PERSON15) And of course, they might be just different instances of the same thing. 
(PERSON17) Yes. 
(PERSON15) And maybe interact with the same automatic speech recognition system. Uh but from the architecture a point of view it have to be different. And and that's it. 
(PERSON17) Yeah, that that make sense so- 
(PERSON15) Okay. 
(PERSON17) So the alph- so the the set up if we have now we are more discussing like the uh, translated or interpreted remote conference. So imagine that we have ten people participating in the same online remote conference in the [ORGANIZATION6] platform. Each of the person speaks different language. And ideally, we would like to see uh,  we would like to hear the the mix of languages. And see subtitles in one selected language. Uh, at each of the end points. So each of the endpoints, the end users will have, this will be a separate channel in the [ORGANIZATION6] platform. This separate general will be connected through its own separate client to the uh [ORGANIZATION4] mediator. Uh, and then, uh, there will be a number of ASR workers. Uh, each labeled with the language it can recognize we will know which of the speakers speaks which language there could be five speakers of [OTHER3], uh, and- So there will be five clients that produce that like populated,  of-  Produce [OTHER3] sound uh packets to, the uh, to the, mediator. The mediator would have many [OTHER3] ASR workers. There could be more then the speakers are which cases like over capacity. Uh, there could be fewer workers,  then how many there are speakers of [OTHER3],  in which case, uh, some of the uh, request will be delayed uhm. But in principle, it would like processes them after that as well. And if uhm not all the speakers of [OTHER3] will be speaking at the same time. Then the fewer workers will be stay- still able to handle that concurrently uh. And each of these ASR workers would do the transcription into into [OTHER3] subtitles  and then there would be one client again per uhm per the endpoint that would would grab the subtitles and and display them, uh, on the on the side of of the end user. So this is for the live set up. <other_yawn/>
(PERSON11) [PERSON17], I think it was a less worker it won't work.  Because if we you less workers, the streams will  just not be stabile, because it would says there is no worker available. And I guess switching like sharing workers between sessions is not possible. So you need as many workers as you have at least as many workers as as you have speakers. 
(PERSON17) Yeah, okay. Thank you. But you mentioned again sessions. So how our sessions related to the channels? 
(PERSON11) In that's up to the design of the- How you want to design it. I mean the easiest thing which is of course will not be the best for resources. But the easiest thing would be one session per channel.  And then you have an independent system for each. The problem then you have that's why you need time stamp that in the end you need to synchronize all the different sessions as, so that the the output the translation can be displayed nicely. Because now at the moment, it only supports like one channel one session. So the easiest thing to extend it would be to have one session per channel. And then at the end march again, the outputs. 
(PERSON17) Mm-hmm, yeah. And the recording is also complicating complicating things,  because then you would meet to have one saver so the say per session and without time stamps you would not know how this relates how to reconstruct the the whole stream. The mix stream, right?. 
(PERSON11) <unintelligible/> time stamps anyway,  if you send  anything to the mediator you have to put in time stamps. Because each measu- message has a times stamps,  so uhm we will have the time stamps in the mediator, because you can not send anything without time stamps. 
(PERSON17) Yeah. So now it is for [ORGANIZATION6] to decide whether they uhm what is the easiest for them,  whether to include timestamps somehow to their, like separate channels for the recording purposes,  or whether it is easier already to implement the client. Uh. So that each [ORGANIZATION6] client behaves as a client for the [ORGANIZATION4] mediator,  and then, and then uh, the- and how to s- do the set up within the platform. 
(PERSON18) Yeah, so we will discuss internally,  and we will discuss internally but maybe contact you, [PERSON15]. 
(PERSON15) Maybe it's better to contact [PERSON14]. <laugh/> Thank you. 
(PERSON18) Ok.  
(PERSON17) Okay. So for, so for the fair we know that there is the simple recording client. And we need you to polish it. And for the uh, for the recording of our meetings within [ORGANIZATION6] we will know later, when when [ORGANIZATION6] talks to uhm to [PERSON14]. And and when when the decision is met. Thank you. So I try to record this later in the in the notes, uh,  and there, for for this session, that [PERSON15] presented. I would just like to ask [PERSON15] or [PERSON1] more to provide the details. So we uh, uh, everybody knows where the code is uh, the slides, if you could upload them somewhere. 
(PERSON15) Yes, of course. I will uploaded the slides in the [ORGANIZATION4] platform sample co- sample connector repo.  
(PERSON17) Yes, yes. And then just add a link here or something like or let me know. 
(PERSON15) <unintelligible/> talk directory. 
(PERSON17) Yeah, okay thank you. And yes, when I was listening to that presentation. Uh, I had this question, whether the platform will seamlessly scale up to the [ORGANIZATION5] use case where we expect about six input channels for six different languages and up to 43 target language subtitles. So uh, I've kind of worried that if we have so many independent machine translation systems connected to uhm to this, the platform may also have some a nontrivial overhead. And this is, this is like this is something that we do not have to resolve fully uh, in the next few months. But within the year, we have to have this resolve properly. Uh, and and- 
(PERSON15) Actually the uhm the system is meant to be able to scale up to to this kind of sizes. I don't know exactly how many worker and interaction we manage during the <unintelligible/> project, but okay. It should work. <laugh/> 
(PERSON17) Yeah, it should work. Okay, that's that's good. Uh. So uh, related question is, if we develop machine translation systems that produce more outputs,  like more target languages at the same time, because there are like benefits at least the saving GPU memory or something like that. So so that you have a uh, we don't have the systems yet. But this is what we are experimenting with in within year one. We would like to have one machine translations systems that can translate from one language into six languages at the same time,  uhm and when I say at the same time. Uh, I mean, uh, that, uh. It is two, two things either it can be like producing one output language at a time. So you ask it six times  and it will give you six different target languages for the same input string,  or you can run these even in in a in a badge on the GPU <unintelligible/>. So you will have the same. Uh, input uhm sentence translated into six languages at the uh, like at once, uhm thanks to the <unintelligible/> if in the GPU <unintelligible/>. And now we have this system. And what would be the best set up to connect this system to the to the platform,  because it would- the same application,  the same code should actually behave as a six workers? 
(PERSON15) Yeah, it's a good question. At the moment, it doesn't match the uhm the fingerprint uhm design yes for example.  If everything has to be sent to the same client, maybe we can think about some work around  but I think that in uhm in in your example you want to send different language streams to different uhm client. Hmm, at the moment, it's not supported in this kind of design. I'm sorry. 
(PERSON17) Yeah. Okay. That's alright. So with this is something that we should keep in mind,  uhm and discuss how to what what is the best solution. And primary we need to have these multi target systems. Yeah,  Okay. So. That was my question. And another question was that what there are some useful tools within your platform or whatever for for long run testing. So if we have some. Uh, some worker, like our machine translation worker,  or our ASR worker. Uh. How do we tested so that it doesn't have any memory leaks and and things like that, whether you have some some like small scripts that do the testing. This this could be useful as well. 
(PERSON15) I have to ask. But I think that we haven't tools like this one. 
(PERSON17) Yeah. 
(PERSON15) I can check it.  
(PERSON17) Yeah. It could be some very simple things like feeding <unintelligible/> that feed something there,  but yeah. 
(PERSON15) Usually we prepare our ehm client, let's say in order to perform such kind of test. But uhm is something uhm we use just once and we drop it ehm <laugh/> 
(PERSON17) Yeah. 
(PERSON15) Sorry. 
(PERSON17) Okay, yeah, yeah. So. If if there is anything, just, please added to this document. Yeah. Then I've recorded here in the notes that the platform distinguishes the batch and online processing mode. We'll definitely use the online mode,  and possibly also the batch mode, and [ORGANIZATION6] will probably use only the online mode. And I also like uh, in in bold words I said there should be no practical limits in the batch mode.  Ehm. Yeah. But this. But this was not really tested. Right? 
(PERSON15) Right. Actually we have worker to work in real time. But I don't know if in the uhm provide example there something ehm ehm serious  <laugh/> about these kind of usage. 
(PERSON17) Long term. 
(PERSON15) The client <unintelligible/> example. 
(PERSON17) Ehm. 
(PERSON15) Also has switch we can help you to emulate real time streaming starting from an audio file.  And this should work  but I haven't tested in the last week, sorry. 
(PERSON17) Yeah, okay. So this is very good to know. So so the default mode for the client fo- So this is now it's hopefully shared you all hopefully start looking at the [ORGANIZATION7] document  where I'm at, uh. I've put here the two uhm specific pieces of code that you mentioned for the clients. The best example is the client <unintelligible/>, right? Is it the right one? 
(PERSON15) In my opinion it's the uhm more complex,  but the more complete example client <unintelligible/>. 
(PERSON17) Yeah. It's- I think it's the one. It's I'm just highlighting it. You agree that this is the one, it should be shared on your screen? 
(PERSON15) Yes, yes. 
(PERSON17) Yeah, uh, and and the default mode for this is the online,  or the batch one? 
(PERSON15) Oh, ehm it has a lot of a common line parameter. It doesn't fault mode. 
(PERSON17) Okay. 
(PERSON15) Today we have gone through the uhm the simpler example, but you can try to uhm verify all the possible options. Are well commented it also has aaa  <unintelligible/>. And okay. You should find all the required information. 
(PERSON17) Yeah. Uh, and and it should also have an option to simulate live streaming from an audio file. Uh not tested recently. So this is something that we should test. And this would be the best way for the testing actually,  so that we connect such a bit direction a clients, to the the running mediator and test how our ASR and  machine translation systems behave. Yeah. Okay,  and the most complete example for the workers is this backend ASR too. As you mentioned. 
(PERSON15) Actually, it's my favorite example. But also you have ehm machine translation example. The audio record example. Okay. We send it out of indication but it's the same. 
(PERSON17) <unintelligible/> audio recording example. Okay. Yeah, so. That's that's it. I don't know I don't know if there are any further questions or comments from anyone else. I'll try to record briefly record what we have discuss about the uh the channels  and recording and time stamps. I hope that I'll be able to recollect.  But this is anyway something that [ORGANIZATION6] needs to discuss directly with uh, with [ORGANIZATION4]. And please everybody attend to the to the requests, the like, uh, the details  and  whatever whatever else is needed. So if if that is all. Nobody has any other questions or comments? Okay. If everybody is fine. Then I would like to thank everyone for uh, participating in the session. I'll try to record. I'll try to extract the recording from this system,  and also to to provide you with with the recording so that uh,  maybe other people who didn't have time to uh, to attend now can can review the tutorial given by [PERSON15] and uh,  and sh- and follow the uh, the instructions to uh, to get the clients and workers running and connecting. Okay? 
(PERSON15) Okay. Thank you, thank you so much for listening me. 
(PERSON17) Thank you and will be in touch with all the individual details. So thank you. Bye-bye. 
(PERSON15) Bye. 
(PERSON18) Thanks, bye, bye.

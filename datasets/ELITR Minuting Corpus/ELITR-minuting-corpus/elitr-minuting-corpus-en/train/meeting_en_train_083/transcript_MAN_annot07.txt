(PERSON12) Hello. I hope you can hear me. Mm-hmm. 
(PERSON8) Hello. I can hear you.
(PERSON12) OK great. That's good. And we have [PERSON14]. So [PERSON14], let's test your connection first. That's [PERSON8], right? So [PERSON14], can you hear us? Does it work for you? Yeah, so we have also [PERSON3]. Yeah. Hi, I can hear someone. Was that [PERSON14]?
() Hello, Hi everyone.
(PERSON12) Yeah, so now it works. Okay. Great, perfect. How yeah. So, [PERSON14], are you also looking and the document, where we are all looking at, the [PROJECT1] Surge organization.
(PERSON14) Yes. I see-
(PERSON12) Yeah, yeah. It's good if, it's good if people are signed in, but it is not required to to Gmail, because then you are not anonymous bats, iguanas sheep and dragons or or tigers. But instead you're a real people. But it's it's more for the revision history, actually, but it is- In any case, it's it's-The most important thing is that you add things to the document. Ok. So the connection to ean seems to work today. Let's hope it will work regularly. Every Tuesday. So welcome on board. And this is also for other, at least, get to know your voice. And we have just the 30 minutes or 28 minutes, actually, to go through things that we have done. So I would really like these meetings to be speedy, and also to get nice nice minutes from those. So whoever is like listening, please volunteer and edit the document to to take note of what the speaker is is just saying at the moment. So there, someone could have edit for me that I have invited [PERSON14]. That's just an example. It is not not important. Okay, so now, thanks for joining. And let's quickly go over the the updates. Uh, I have uh, I'm not finished with my updates yet. But actually, I'm always lost in what I have done. So it is that is I believe that you are probably equally lost in what you have done. So that is why we have these meeting, so that everybody is aware of of the progress. So I'm mainly supervising the creation of the [PROJECT1] test set and the edible SLT test set. And the main thing that I wanted to say today is that the set of tools that we have at hand is nicely growing. I need thanks to several people of from, sev- several people from you. So if anybody is processing any audio, and we have [PERSON11] here, right? No, [PERSON11] is gone. Where is [PERSON11]? So I will, I will have a separate call with [PERSON11], anyway. So I'll tell him. So when when anybody has long audio file then and existing transcript. Then there is a tool, ELAN, which can easily, which can be used to easily segment this long audio into sentence long units. Then there is the forced alignment, which can be used to identify words in that for Czech and English. And these this word level alignment can be validated by showing it as karaoke style subtitles. So the, it's colour coded which words are just being uttered. And then also this is, there is another conversion script, which creates the files for SLT evaluation. And if the fai- forced alignment fails, then there, the ELAN tool can be used even for manual segmentation at the word level. So for manual identification of where the words are. So this is what what now need to be like, put to put to use for the [PROJECT1] test set preparation. And then another set of tools is very slowly growing for the text level processing uh, so, uh, I have, uh, like uh, refined bin based editor of tool 3 or 4 files in multiparallel corpus. Multi- parallel test set. So this should be used regularly when <unintelligible/> our test set, test sets. And also a simple preprocessing for-Yeah, so this this this is where I would really appreciate if someone was was taking note, because I cannot speak and write at the same time. So, simple script that runs sentence segmentation. With some errors, and then sentence alignment again with some errors for pairs of files. Uh, so this sentence segmentation and <unintelligible/> together are deeply processing, which is needed before you start refining the uh, the test set. So this is, yeah. This, these tools are necessary for any evaluation. Okay. So that's, that's what I've realized now. Maybe I'll realize more things that I've worked on. But let's move to [PERSON3], so that we don't spend all time on me.
(PERSON3) Hi guys. So lately, I was working on setting up client audio visualization. And now it's pretty working.
(PERSON12) Yeah, so can we test it, can we see it in any of our like Monday test sessions?
(PERSON3) Ehm, yeah, but I I, you will actually so, that visualization would be on anyone live screen. In my browser.
(PERSON12) Oh, so it's web based, right?
(PERSON3) Yeah, yeah.
(PERSON12) And is this, yeah. Is that, that's that's an option, but I think it, it will be s-It can be more convenient if it would run also in simple like separate windows or or window, like attached board.
(PERSON10) Yes, so-Yes, if I can interrupt, yeah, so the visualizer for the for the [ORGANIZATION7] can can run on-
(PERSON12) Anywhere.
(PERSON10) Yeah, it is web based but but it doesn't really matter because you can just-There is a server which streams it and the web client, so.
(PERSON12) Ok.
(PERSON10) So if you want you can, you can, you can like stream it to multiple client at once or you can do whatever you want with with the data.
(PERSON12) Ok, yeah, yeah, yeah. So, so the the main reason why I think we needed this is exactly for a live sessions when we need to set up all the, all the processing pipeline. And this is to validate the source channels. So the one who is operating the system, should be able to to check if all the sources, in a [ORGANIZATION8] environment, are there at at the right like volume. And that's that's what the visualizer is for. So ideally, our cruise control set ups should include this and spawn it as processes on the main machine, which operates the whole pipeline. And we would immediately see if something is working or or not. So that's the, that's the idea. So, yeah, so [PERSON3] I didn't really want to interrupt you, but let, I did. So you work on <unintelligible/> for mock conference. Transcript generation, and you're playing with, yeah yeah, <unintelligible/> workshop. And also, I would like to add one more like a to do item. To clean up the the set up. So that we have, so we have now multiple folders in cruise control set up. And I would really like to have one uh, release of of that. So to say, in like <unintelligible/> engineering style. And that release would be a set up which illustrates Czechs ASR into other thing, English ASR into all the things. Maybe even French ASR and it should have all the bells and whistles that we have so far. So it should have eh the joints standard <unintelligible/> locks. It should have the online visualization of input channels. Uh, if if we already have [ORGANIZATION8] set up. There will probably that's should rather be like separate one, so that it is not a cluttered if you go for a particle set up. But these should be like the, you should have now version. So so creative version kind of, obvious it should still if on the same place, but it needs to be polished. And all the bells and whistles merged across the various there is. So it should automatically create the transcripts with segment-level timestamps, and or even more level timestamps, so the the lock files that SLT then digest so all this nicely wrapped. So this is, this is what I would like to uh, to work on, in addition to to what you are working now on now.
(PERSON3) Yeah.
(PERSON12) Yeah.
(PERSON3) So I think <unintelligible/> everything.<other_yawn/>I was wondering if <unintelligible/> because I feel like I'm quite free these days so, I I will do it <unintelligible/> more task bec- because <unintelligible/> and I feel depressed.
(PERSON12) Oh yes, I have the same thing actually. And also realize the same thing with my kids. They have started receiving less work, like these last two weeks than the three weeks before, and it it is much worse for them, because they are like doing crazy things instead of doing something useful. So yes, exactly. So there, the one thing that would be very good would be uh, the segmenter, and also, maybe help the students of mine. I have two students. Suni is one of them, and then another Czech student, but he was also very passive so far, because he was busy with other school duties. Who should work on the segmentation including phoneme representation? So they could probably uh, who benefit from some like supervision, especially like I'm offering that regularly, but they are not asking much and also I'm overloaded. So if I could like ask you to help them, that would be a better version of the of the segmenter. Or you can work on a separately on on the segmenter on your own. But there are many bugs and and the segmenter is is now the critical point.
(PERSON3) Yeah. So like could <unintelligible/> the segmenter, segmentation in in <unintelligible/>
(PERSON12) Yeah, so let's let's leave that like outside of this call because-
(PERSON3) Ok ok.<other_yawn/>
(PERSON12) So I'll I'll send you what I sent to them and then you can ask for more.
(PERSON3) Yeah yeah, ok.
(PERSON12) So so then the polishing, the polishing of all the set ups. So that it is easy to to launch. And most importantly, so that it is easy to diagnose what is wrong. So we are now, there is no, no Monday sessions now, but there is, for example, this particle called, this very call can serve again as a test case. And I've seen in one of the last calls that like just for fun. I I did the Czech ASR into many languages. For call I had with the colleagues of mine. And there I saw again how bad the segmentor is. On the [ORGANIZATION5] seminar I realized that one overloaded machine can kill the whole show. And it took us 20 minutes to find it out. So that's something which is very bad.
(PERSON3) Yeah, yeah.
(PERSON12) So these-
(PERSON6) I knew it. Much sooner than we than the 20 minutes. 
(PERSON12) Yeah, yeah, but but then I was not able to like receive that message. And also I didn't know what to do. Like it's this happens easily when the main operator which was me, is not skilled enough and when the main operator is like overloaded and there is no was to come to the person physically, and like knock on his shoulder and say,"Hey, you should look at this window, because [PERSON6] is telling you what to do."So, yeah, yeah. So this, it's it's part of that is like communication issue. Another problem is that we do not have the diagnosis tools. So how did you know, [PERSON6] that that was the problem?
(PERSON6) Because I remember that [PERSON3] add the bug few months ago. And and we and we check it and found it, and but he but he forgot to run it to prevent this bug again.
(PERSON12) Yeah, yeah. So-
(PERSON3) You are talking to me?<unintelligible/> the ASR with more number of source.
(PERSON6) Yes. To be the only process which is running on the machine on which are running-
(PERSON3) Yeah, yeah.
(PERSON12) Yeah. But that, ok. So there is , there is some like configuration bug, how do we ensure that we avoid these? I don't think it is possible. So one thing is obviously to use the issue tracker. Let's use the issue tracker in GitHub for cruise control. That's what it's for. And I believe that's probably also recorded as an issue. Or could be very likely. And there is surely an issue recorded for the subtitler. So.
(PERSON6) But I thinks there should be one simple script which runs the [PERSON7] ASR on our cluster in the correct way.
(PERSON12) Yes.
(PERSON6) Right now, we don't have it. So [PERSON3] must do it some with several steps and ensured manually.
(PERSON12) Yes. So [PERSON3] this is another thing that you should you should do. So maybe start proper like'Bugzilla' issue tracker, but this even for yourself, into the issues in GitHub for cruise control. The polishing should be there as well, and get to get used to this standard programmers daily routine of like checking the the list of issues and ticking the them off. Obviously, you do not need to do any <unintelligible/> requests. We we do not do a coder use. But it could we could even do that, if that is if if we would have more time for that. But I do not think we have any one to review that. [PERSON6] should be working on on the research, and I'm too busy to provide any sensible reviews myself. But let's use the the issue tracker. So and and it's, yeah. I know that this requires a change of the mindset. And I'm unable to do this mindset, I'm always like avoiding this this daily routine. So [PERSON3] if you have similar problems, as I have with with that, uh, then, say so, and we will like try to make it more interesting, or trying to find way how to how to go around it. But it's it's <unintelligible/> the same thing, the polishing of the pipeline, so that everything is easy. Everything is as small as short as possible. And everything is tested, and everything is run like, there is diagnosis for everything. So the the diagnosis is totally critical. And I still do not know what to do when I like run the preparatory script, I see that giant pipeline. And then I run it. And after a minute or two. It says error. So there, I do not really know how to quickly like shorten the pipeline. And test which part is has failed. And which part is is OK. And this error can be any worker anywhere. So I have to go into the log there, see which of the log files is the shortest one, which is an indication that this one is probably failed. Then there is often many of those in the log there I no longer see the hierarchy of the of the commands. So clearly it is numbered, but it is still, for example, all the rainbow systems are like at the same level. So it's it's very difficult to-Still what I would really like to see, and that's just a vision, would be this uniques command printed again. And one line colored in red, indicating like this one killed the whole thing. I don't know how to do that. But it's, this is what I would like to have. Ok, yeah. Thanks. So that's, that's good. Let's log there for, yeah. So if you have any questions, like where is the path for dry run workshop, please make it a comment, Google comment, to someone who knows it. So this log there for the dry, the dry run workshop. [PERSON13] has talked about it, we need to ask [PERSON13] and [PERSON9], actually as well, I I think. Or or you are the one who who recorded it, right? So you should know as well. So what do we mean by dry run workshop? 
(PERSON3) The last dry run workshop which <unintelligible/> online <unintelligible/>.
(PERSON12) The online? Ok. So that I don't know. Yes, so we need to ask [PERSON9]. [PERSON16], yes, [PERSON16]. Yeah, [PERSON16] recorded it. And he said it's on his machine. Yeah, so let's ask [PERSON16]. Thank you. So where is the path. Yeah, so.
(PERSON3) So in the last e-mail concerning this ASR transcription I replied to [PERSON16].
(PERSON12) Yeah, I actually make a comment here, in in the Google doc. Ok, thank you. So then, [PERSON13] is not here. Let's quickly move forward. So so I'm supervising him and [PERSON13] should be in close touch also with [PERSON11], hopefully. And [PERSON11] is here, so [PERSON11]?
(PERSON11) Yeah, hi.
(PERSON12) So please summarize what what you have work on.
(PERSON11) Okay, I have prepared 21 languages test set. Which is I have upload there 6 languages on the GitHub. And remaining will be uploaded very soon. Because I have been some data. So most probably. And all these data are extracted from the <unintelligible/> court of European. Yes. And from [ORGANIZATION9]. So.
(PERSON12) Yes. Yeah, so please put them into subdirectories of of these names. And include the read me with the original link. So that we, not necessarily for each of the files, you can do that as well, but at least for the Web site, so that we know where it comes from roughly, and that immediately tells us the domain uh and so on.
(PERSON11) Sure -
(PERSON12) Because so the [PROJECT1] test set will include domains of computational linguistics, and auditing. And we need to immediately see, which is which.
(PERSON11) Okay, I will up-, update <unintelligible/>
(PERSON12) Thank you. And also provided transcription of workshop. What do you-Sorry that's that's [PERSON13]. Yeah, so several languages uploaded, yeah. So and I would like to check the status of annotators. So how many are you in touch with, how many have even in the last 2 weeks actually. How many have asked the question. How many are waiting for what. Or how many are waiting for us, or how many are we waiting for? So I would like you to have a an overview of this. So do you have an idea.
(PERSON11) No, not yet.
(PERSON12) Yeah, so please, please make sure-
(PERSON11) Ok, I will discuss with you in the evening.
(PERSON12) Yes, yes, exactly. Yeah, but please please make sure to to have some overview because this is, everybody can could in principle make use of these annotators, similar [PERSON13] has access to to annotators. 
(PERSON11) Ok.
(PERSON12) So if we spot some strange language document that could be useful. You are the person to to contact, and you will then know who you could contact to get it done. And with each of the annotators. You are the person to supervise the the process. And you were, you had something technical issues see in the document, I highlighted the tools that you should be aware of. Because these tools are useful for the annotators, depending on what type of resource they find. If they find audio, they will needs these ELAN scripts, ELANs and scripts and then if they are Linux skilled they should use the <unintelligible/> editor and we have this simple pre-processing for sentence segmentation. Plus sentence alignment. This was by me. You know about it already. And you have maybe your own pipeline, which could be even better than mine. But well. If, so feel free to improve mine. My, my set up there.
(PERSON11) Ok, I will check.
(PERSON12) Yeah, so we need to stream line this. So we need to to be very quick, if some person random person says " I have a Kazakh, I I speak Kazakh, I have a test set which could be of interest for you file, which could be of interest of you. Then, on that day, ideally, he should receive it pre-processed by us. And in in 5 days or a week from that. We should be asking him, have you succeeded in in polishing it. And we should process in and include it in the in the repository.
(PERSON11) Ok.
(PERSON12) So this is really be quick and with little afford, little manual afford, on your side for every single person. So you need to automate thing for yourself. 
(PERSON11) Okay. 
(PERSON12) Yeah, okay. Thank you.
(PERSON11) Ok, thank you. 
(PERSON12) Let's quickly move to [PERSON6], because we still have [PERSON10]. Yeah. So [PERSON6], are you still here?
(PERSON6) Yes. Can you hear me?
(PERSON12) Yeah.
(PERSON6) So I evaluated the 2 new ASR workers. For edible SLT. And I found out that they are not much better than before. Nice multitude format and implement the MT wrapper and online text for events and so on.
(PERSON12) Yeah, yeah. Sorry. For these ASR workers, were any of them offline? Or all were online?
(PERSON6) Both are online.
(PERSON12) Yeah, and and [PERSON5] unfortunately, indicated that there is no improve, like he he cannot run any offline systems that would have any chance to be better right?
(PERSON6) No.
(PERSON12) So at this point, we are stuck with with the ASR that we have from [ORGANIZATION1] and which is pretty bad. I've asked [PERSON4] and [PERSON4] is starting to train his implementation the the just per or <unintelligible/> set up for English. And [PERSON8] is also hopefully trying to train another [PROJECT6] based ASR. So we could have our own ASRs.
(PERSON6) Ok.
(PERSON3) You've been trying English ASR?
(PERSON12) Yes, yes.
(PERSON3) Ok.
(PERSON12) And for these tests, this is for edible SLT share task. I've kind of given up on the online aspect. So uh, I do not know, well actually both of them. I think that both [PERSON4] and [PERSON8] will only have like transcripts, without time stamps. But that's fine. Let's let's focus on the on the ASR quality, which is the first criterion of the shared task anyway. 
(PERSON8) I can decode also the time steps, if it's-
(PERSON12) Ok, yes, that would be useful. So that would be- So ideally, we need, so talk to talk to either [PERSON15] or [PERSON14] in person but that would be slow probably, because the communication to <unintelligible/> is often <unintelligible/>. And or have a look at the edible SLT share task, I'll send you the link. Ideally, we need time stamps for when the word was uttered that's source as, like start and end. And we also need the third timestamps, timestamp which indicated when that was recognized. So the ASR system can be run with a special T script. Or T tool at the end. And this T emits the standard output and adds timestamps as well. So this is the way, or talk to [PERSON3]. This is the way that we record how fast the the recognition is delivered. And it is, it is OK for the shared ask to like have the integration fake. So you run it separately, you will recall your speed. And then we will say that there is absolutely zero processing time to pass it to the MT, and the MT will again add its time. And even if we do not have any timing. That's good. Let let's go for just the recognition and translation. Someone's writing that. Excellent. Thank you. Yeah, okay. So sorry [PERSON6] for for the interruption, but this was exactly one of the good points that we have the meeting. So that all of you know about each other now.
(PERSON6) Yes. And I'm going to reformat the MT wrapper and online text <unintelligible/> events and so on. The point is to have the brief format of online text flow. Because right now many sentences are being repeated. And it's and it's difficult to analyze them.
(PERSON12) Yeah.
(PERSON6) And after I will do it, then I can easily implement the scripts for Flicker and delay evaluations. Cause right now, I have some bugs there.
(PERSON12) Yeah.
(PERSON3) Is that current <unintelligible/> of main text flow is stabile and stabile to use.
(PERSON6) Yes, yes. And I have I've fixed the bug with the installation.
(PERSON3) Yeah, yeah.
(PERSON12) Ok, great. Yeah, great. Thank you. So how many people have here who have not spoken. We have [PERSON14]. But I'm not sure if that connection will work, and we have [PERSON8], and [PERSON10], right? So maybe, [PERSON8], if you have something more to say, so that, with to start.
(PERSON8) Yeah, so we just do quickly right up. I I've been running some experiments. For my diploma thesis about the Czech ASR. I've done like domain word classification and accuracy, so I have several adaptation techniques and I'm computing how many of the domain words that identify are recognized by each of these adaptation levels. And then since like two days ago, I have been working on training the [PROJECT6] co- [ORGANIZATION4] common voice English ASR. As we already discussed briefly. So right now it's on like sixteeth iteration of five hundred. And it's running on like 6 GPUs. So I think should be done like tomorrow afternoon. I guess. So-
(PERSON12) And is it just, is it just common voice or is it something else as well?
(PERSON8) No, this just just data from common voice just 300 hours. Just, it just like vanilla set up so because I, from what I understand, there is quite like time pressure on-
(PERSON12) Yes, that's true. Yeah, yeah. So it's good to have, it's good to try this one. But [PERSON4] was also telling me that this common voice contains a many, a many utterances, but of the same sentences. So it's that many people read the same sentences. And that in your case that should not be that of big of a problem, because you do is for the acoustics, right. So the the fact that the vocabulary is limited is not to too critical. If you then use a large language model right? Or no?
(PERSON8) Yeah, well. Yeah, then I just now, I use the baseline language model, which is train only on the transcripts. So there this issue would arise. But I I plan to train a separate like larger language model and also create a larger lexicon. So that the model capacity increases this way.
(PERSON12) Yeah. The, yeah, that's very important, because the vocabulary is very small. Or the number of sentences of common call is is very limit, very small because it's it's repeated sentences. 
(PERSON8) It would also be, maybe I quite helpful if the language model would be trained on the similar text as the MT model. So then maybe I can coordinated this [PERSON6] and-
(PERSON12) Yes, yeah yeah. Definitely. So, so you should you should include everything. Everything that you you can. So common common voice, and also maybe just the huge English text, the <unintelligible/> size for example. The English side of <unintelligible/> two point zero. But, that will make it actually, double check the edible SLT test set. Double SLT test webpage. Because that indicates which corpora are like license, or are ok for for the constrained run. And it is OK to go beyond that. But if we are already limiting ourselves, it is more important to limit ourselves to uh, to what is the allowed for the common, for the constraint wreck. 
(PERSON8) Ok, yeah-
(PERSON12) So I'll I'll tell you or ask me again. So search for edible SLT non-native task.
(PERSON8) And maybe one last comment, there is if, if we are not running in online set up, then there is quite interesting feature from [PROJECT6], that I can predict after each word, I can predict a silence token. And its duration and if the duration is longer than some threshold than it's very good indication that there is end of sentence. So this can really help with segmentation. Like the punctionation of the output. So maybe I can also pass this information to [PERSON3] or and it can be I think very useful to the segmenter model.
(PERSON12) Yeah.
(PERSON8) Yeah, ok so that's all I think from me.
(PERSON12) Ok, thank you.
(PERSON3) I I would like to make a comment on. I have a vision of what the Czech ASR. So the very end, it would be good to have the segmentor integrated with the Czech ASR, like the once you <unintelligible/>. Possible?
(PERSON8) Well this is like kind of adding an acoustic information to the segmentor. Because, yeah it predicts the silence after each word. And sometimes the duration of the silence is just zero. But sometimes it's like quite I don't know 20 milliseconds, or something, which is usually a big indication, that there is like end of sentence or at least a coma. So this could, like a kind of be like build in segmentation of the output. But I think there is still like some NLP needed to be done like it's it's not definitely not like it does not solve the task, but it can help quite a lot I think the segmentor. But I, yeah, I haven't played too much with it, because I just recently like discover it and so, maybe I will have more info in like couple weeks.
(PERSON3) Ok. Thank you. 
(PERSON12) Yeah, ok, thank you. So we are already running late. Let's, let's try to be as quick as possible. [PERSON10]. 
(PERSON10) Yes, so I haven't done very much this week. I have just finished the evaluating of my models. And so I I can confirm that-
(PERSON12) Is that 36 to 36?
(PERSON14) It's actually 42-
(PERSON12) 42 to 42, yeah.
(PERSON14) Which is, it's working. So the scores are a bit lower than [PERSON6]'s scores for the IWSLT test set. But only marginally lower, so so there is the benefit that that you have one model for all these languages. So, so perhaps. Yes, so at the moment I don't have anything to add so maybe maybe you could tell me like what to do next. Or or if we want to what do we want to do with it.
(PERSON12) Yeah, so. It's integrated, like in the sense that we can really use it in the in the daily tests of the whole set up?
(PERSON14) Yeah, so I'm not sure about it. So how does it work in their repo? Does, does it work like- Should I integrated into the-
(PERSON12) You should-
<other_yawn/>That's not not, that is not needed.
What is needed is that [PERSON3] knows how to run it, and he codes it into the scripts into the set up scripts. So part of the tools are running, and we cannot run them ourselves at all, such as the [ORGANIZATION2] systems, and we still use them in the pipeline. But for the tools that we know how to start. We really should have like launchers or even installators and launchers within the cruise control repository. This is true for the [PROJECT6] set up. But it is not true for the [PERSON7] set up and our uh, at our machines. This is one thing that [PERSON3] should add. And we also need that for your model. And we probably do not have it for for the [PROJECT2] model, right [PERSON6]? When you when you run the [PROJECT2] workers on our side.
(PERSON6) In cruise control there are scripts which run the worker on the coster.
(PERSON12) Ok, yes so this is, this is what what needs to be there. I don't know where these scripts are in cruise control, but yeah.
(PERSON6) Today installation and move [PROJECT2] and to do models.
(PERSON12) Yes, yes. So this is exactly what-So please put the path fo- how it is done. For [PROJECT2]. To, here to the document, so that [PERSON10] knows what to mimic with the with the - is it still [PROJECT5]?
(PERSON14) Yes.
(PERSON12) Yeah. Because I I I've realized that [PROJECT5], or someone told me that [PROJECT5] is kind of discontinued.
(PERSON6) But we have, we have connected [PROJECT5] model to mediator. The <unintelligible/> models.
(PERSON14) Yes.
(PERSON12) Yeah.
(PERSON14) Yes, I have just discovered that they have they have just discontinued [PROJECT5].
(PERSON12) Yeah, but that's not a problem. Let's let's live with that. Yes. Yeah, so so please integrate it so that it becomes an option and uh, so that we actually can really provide all the target languages, all the 42 languages with the subtitles, because that again will create the rainbow messages, much more verbous. And we need to test the pipeline with this level of of vol- like volume. So all the-We should really increase the set of cover languages, so that any Monday seminar, or even a Czech [ORGANIZATION5] call could have subtitles in Kazakh or whatever we have.
(PERSON14) Hmm-hmm.
(PERSON12) So this, this is for you and [PERSON3] to to discuss. Right?
(PERSON14) Yeah and maybe maybe also, I would like to add that, that I still have the TPU computational capacity. So if [PERSON8] needs some some help in in making the the ASR train faster, then maybe I could, I could help. So maybe-
(PERSON12) That's the, maybe not [PERSON8] but [PERSON4]. Because [PERSON4] is also like he is still doing experiments for his <unintelligible/>. So he doesn't want to focus too much on on this. And also he knows that like his- He has some English system th- systems trained, but his are more GPU hungry. It is N to N neural approach. Where is the [PERSON8] set up is only, the does the hybrid approach. So it would more help [PERSON4]. So if you if you have the e-mail of [PERSON4], then please contact him directly.
(PERSON14) I'm not sure, could you, could someone send it to me?
(PERSON12) Yeah, yeah, yeah. Or I'll do. Yeah, great. Yeah, great, thank you. So I'm happy that people are taking notes. And also, [PERSON10], you maybe getting some tasks from [PROJECT3]. So will see. So please, like balance it and let us know how you're doing the sum-This will be mainly by [PERSON1], I I guess. Ok. And then we still- I I still wanted to mention something. And I forgot what was that. Yes, your models, [PERSON10]. Your models are also needed by [PERSON11]. So [PERSON11] are you still online? [PERSON11], can you hear us?
(PERSON11) Yes, yes.
(PERSON12) So, so you need to use these models for the back translation. So we need to gather all the monolingual data, in all the 42 languages. And these should be translated to English with the models by [PERSON10]. And that is what the TPU units, TPU units should be also useful for. Because it should be really large data. You should get large Romania, large Serbian monolingual data. Any anything at all. And this should be back translated into English. 
(PERSON11) Okay. 
(PERSON12) So the the sooner you have anything, and the sooner you stream line this again. So that it's easy for for you to to ship it to to [PERSON10]. Well, both of you are on the same disk. So you just agree where where to put it on the [ORGANIZATION6] disc, then, then just the better. So we- This should, please [PERSON11] create like a directory of back translated text or monotext with back translation. And you should [PERSON11], you should populate it. And [PERSON10] is populated with the translations.
(PERSON11) Ok, I will do it.
(PERSON12) Yeah, ok, that's what I wanted. Thank you. And we have 3 minutes or less for [PERSON14], so beware the call will be suddenly interrupted. At any point but if [PERSON14] can still hear us, then let's use these few minutes to to quickly hear of ... about what [PERSON14] is working on. If that works. Yeah, so that's the the connection to <unintelligible/> if often often unstable like it, it seems connected, but.
(PERSON3) Oh so [PERSON12], could you comment the path where I should <unintelligible/> conference which I interpreted.
(PERSON12) This, well, anywhere, I don't have any place for that, so where should we put it. Ehm.
(PERSON3) So what think I have to do is that I have to manually solve the audio according to the language, because if that <unintelligible/>. So, it was terrible. And because I, that's why I <unintelligible/> ASRs, because...

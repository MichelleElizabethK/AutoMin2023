[PERSON8] bylo navrženo místo PCA dělat ICA a chtějí s ním na tom pracovat další dva lidi. Uvažuje o práci na vkládání slov a vkládání vět. [PERSON8] uvádí, že u ICA je potřeba najít nezávislé, které nemusí být kolmé, ale potom tam není pořadí od nejdůležitějších k těm nejmíň důležitým. Všechny jsou stejně důležité a zkoumá se tam korelace se vším možným. Podle [PERSON8] se jen nahradí jedno písmeno a funguje to úplně stejně. [PERSON6] s [PERSON4] to v nejbližší době chtějí vyzkoušet.
Podle [PERSON8] by se porovnání balustrád s překladem založeným na frázích a s Hiero možná dalo zkusit. [PERSON6] s [PERSON8] se nakonec domluvili, že by se to dalo zkusit.
[PERSON8] uvádí, že začali dělat, že mají NMT a v tom trénují transformer z angličtiny do různých jazyků, třeba do pěti různých jazyků. Dekodér je jen jeden, první slovo je kód z toho jazyka, takže podle toho kódu jazyka je jasné, do kterého jazyka má překládat. Protože je to v dekodéru, tak ten enkodér je úplně nezávislý na tom konkrétním cílovém jazyce, proto se bude učit lepší syntax. Dělají probing a balustrády, ale zatím to moc nevychází, že by to bylo lepší.
[PERSON12] sděluje, že googlové měli články o NMT, že zkoušeli reprezentace na všechny možné tasky a mají něco, co překládá ze sto dvou jazyků do angličtiny. Mají tam crosslinguální tagging. Podle [PERSON6], když chcete multilinguální reprezentaci jazyka, tak BERT je na to lepší. [PERSON12] uvádí, že mají jeden multilinguální enkodér ze sto dvou jazyků do angličtiny. A ten enkodér probujou. Potom mají ještě jeden dekodér na spoustu jazyků, kde enkodér je angličtina. [PERSON12] [PERSON8] pošle tyto články nebo link na blog, kde jsou uvedené. [PERSON6] říká, že multilinguální MT není výrazně lepší, než ten multilinguální BERT, takže dále budou pracovat na multilinguálním BERTovi. [PERSON6] má doplnit další data do fastelinů.
[PERSON12] udělal vylepšení multilinguálního BERTa, který se učí hromadný jazyk a pomocí adverseriálních klasifikátorů se odnaučuje rozpoznávat jazyk. Povede se ho rozbít tak, že mu neklesá to z toho jazykového modelu, ale nejde na něm natrénovat language ID. Takto získaná reprezentace není crosslinguálnější, než ten multilinguální BERT tak, jak je. Na vyhledávání vět funguje tento vylepšený stejně jako ten nevylepšený a ještě ho u toho vyhledávání zkusil na jiných datech naučit lineární projekci (projekci mezi průměry skrytých stavů). V průměru přes všechny jazykové áry, když je ten BERT tak, jak je, tak na 70 % přesně je možné vyhledávat ty věty. Když se to vycentruje, tak na 83 %. Když umí tu projekci, tak na 98 %. Je to pořád na 3 000 vět. Na odhadu kvality toto nefunguje, korelace se zvedla z 0 asi na 0,2. Rozumné systémy mají korelaci 0,5. Podle [PERSON6] je to zlepšení.
[PERSON12] měřil, jak se klastrují centroidy, jak připomínají jazykové rodiny. Podle obrázků to vypadá, že je to připomíná hodně, takže to kvantifikoval. Výsledek je, že conradtubův více separuje ty jazyky, ale ví to o nich míň. Přestože nní možné dobře klasifikovat ten jazyk z vylepšeného BERTA, tak to dobře klastruje podle těch jazykových rodin.
Dále [PERSON12] dělal word alignment, a když použil ty projekce, co se naučil na těch průměrovaných stavech, tak ty ten alignment zhoršují. Ty věty jsou reprezentovány také průměrným stavem. Centrování těch reprezentací s tím alignmentem nic nedělá. Opět platí, že původní i vylepšený BERT fungují stejně a condratubův funguje hůř.
Dále [PERSON12] naprogramoval expectation maximization na nějakých datech stranou. Udělal alignment, na seřazených slovech se naučil projekce a udělal ty alignmenty s těmi projekty. Zatím to na náhodných věcech, kde to pustil, přidává asi procento. Věří, že tyto projekce, které získá, může použít v metrice BERT score (metrika na normální evaluaci). Na odhad kvality mu ale vůbec nefunguje. Doufá ale, že z toho dostane tu projekci, která je vhodná pro ten alignment.
[PERSON12] trénuje s normálním BERTEM. Udělá klasifikaci jazyka, odečte centroidy, a pak to použije.
[PERSON12] začal psát overlive. Společně se domluvili, že [PERSON12] zkusí napsat short paper a dají to do archive 7. nebo 8. listopadu.
Lidi od [PERSON7] se snaží zjišťovat věci o sémantice a vymýšlí nějaké nové patterny, které jsou jako long is opposite of ask. Zjišťují, když zamaskují tyto věci v nějakých šablonovitých větách, jestli jsou schopní to z toho získat. Podle [PERSON12] je potřeba to zmínit v knize.
[PERSON13] psal, že chce napsat topic. Zatím si nevybral, kdo chce, aby ho vedl, nevybral si ani téma. [PERSON8] si přečte jeho články a napíše mu nějaké témata, aby si vybral.


